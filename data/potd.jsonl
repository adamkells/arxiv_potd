{"title":"DualCast: Disentangling Aperiodic Events from Traffic Series with a Dual-Branch Model","abstract":"Traffic forecasting is an important problem in the operation and optimisation\nof transportation systems. State-of-the-art solutions train machine learning\nmodels by minimising the mean forecasting errors on the training data. The\ntrained models often favour periodic events instead of aperiodic ones in their\nprediction results, as periodic events often prevail in the training data.\nWhile offering critical optimisation opportunities, aperiodic events such as\ntraffic incidents may be missed by the existing models. To address this issue,\nwe propose DualCast -- a model framework to enhance the learning capability of\ntraffic forecasting models, especially for aperiodic events. DualCast takes a\ndual-branch architecture, to disentangle traffic signals into two types, one\nreflecting intrinsic {spatial-temporal} patterns and the other reflecting\nexternal environment contexts including aperiodic events. We further propose a\ncross-time attention mechanism, to capture high-order spatial-temporal\nrelationships from both periodic and aperiodic patterns. DualCast is versatile.\nWe integrate it with recent traffic forecasting models, consistently reducing\ntheir forecasting errors by up to 9.6% on multiple real datasets.","authors":["Xinyu Su","Feng Liu","Yanchuan Chang","Egemen Tanin","Majid Sarvi","Jianzhong Qi"],"url":"http://arxiv.org/pdf/2411.18286v1","published":"2024-11-27"}
{"title":"DualCast: Disentangling Aperiodic Events from Traffic Series with a Dual-Branch Model","abstract":"Traffic forecasting is an important problem in the operation and optimisation\nof transportation systems. State-of-the-art solutions train machine learning\nmodels by minimising the mean forecasting errors on the training data. The\ntrained models often favour periodic events instead of aperiodic ones in their\nprediction results, as periodic events often prevail in the training data.\nWhile offering critical optimisation opportunities, aperiodic events such as\ntraffic incidents may be missed by the existing models. To address this issue,\nwe propose DualCast -- a model framework to enhance the learning capability of\ntraffic forecasting models, especially for aperiodic events. DualCast takes a\ndual-branch architecture, to disentangle traffic signals into two types, one\nreflecting intrinsic {spatial-temporal} patterns and the other reflecting\nexternal environment contexts including aperiodic events. We further propose a\ncross-time attention mechanism, to capture high-order spatial-temporal\nrelationships from both periodic and aperiodic patterns. DualCast is versatile.\nWe integrate it with recent traffic forecasting models, consistently reducing\ntheir forecasting errors by up to 9.6% on multiple real datasets.","authors":["Xinyu Su","Feng Liu","Yanchuan Chang","Egemen Tanin","Majid Sarvi","Jianzhong Qi"],"url":"http://arxiv.org/pdf/2411.18286v1","published":"2024-11-27"}
{"title":"DualCast: Disentangling Aperiodic Events from Traffic Series with a Dual-Branch Model","abstract":"Traffic forecasting is an important problem in the operation and optimisation\nof transportation systems. State-of-the-art solutions train machine learning\nmodels by minimising the mean forecasting errors on the training data. The\ntrained models often favour periodic events instead of aperiodic ones in their\nprediction results, as periodic events often prevail in the training data.\nWhile offering critical optimisation opportunities, aperiodic events such as\ntraffic incidents may be missed by the existing models. To address this issue,\nwe propose DualCast -- a model framework to enhance the learning capability of\ntraffic forecasting models, especially for aperiodic events. DualCast takes a\ndual-branch architecture, to disentangle traffic signals into two types, one\nreflecting intrinsic {spatial-temporal} patterns and the other reflecting\nexternal environment contexts including aperiodic events. We further propose a\ncross-time attention mechanism, to capture high-order spatial-temporal\nrelationships from both periodic and aperiodic patterns. DualCast is versatile.\nWe integrate it with recent traffic forecasting models, consistently reducing\ntheir forecasting errors by up to 9.6% on multiple real datasets.","authors":["Xinyu Su","Feng Liu","Yanchuan Chang","Egemen Tanin","Majid Sarvi","Jianzhong Qi"],"url":"http://arxiv.org/pdf/2411.18286v1","published":"2024-11-27"}
{"title":"DualCast: Disentangling Aperiodic Events from Traffic Series with a Dual-Branch Model","abstract":"Traffic forecasting is an important problem in the operation and optimisation\nof transportation systems. State-of-the-art solutions train machine learning\nmodels by minimising the mean forecasting errors on the training data. The\ntrained models often favour periodic events instead of aperiodic ones in their\nprediction results, as periodic events often prevail in the training data.\nWhile offering critical optimisation opportunities, aperiodic events such as\ntraffic incidents may be missed by the existing models. To address this issue,\nwe propose DualCast -- a model framework to enhance the learning capability of\ntraffic forecasting models, especially for aperiodic events. DualCast takes a\ndual-branch architecture, to disentangle traffic signals into two types, one\nreflecting intrinsic {spatial-temporal} patterns and the other reflecting\nexternal environment contexts including aperiodic events. We further propose a\ncross-time attention mechanism, to capture high-order spatial-temporal\nrelationships from both periodic and aperiodic patterns. DualCast is versatile.\nWe integrate it with recent traffic forecasting models, consistently reducing\ntheir forecasting errors by up to 9.6% on multiple real datasets.","authors":["Xinyu Su","Feng Liu","Yanchuan Chang","Egemen Tanin","Majid Sarvi","Jianzhong Qi"],"url":"http://arxiv.org/pdf/2411.18286v1","published":"2024-11-27"}
{"title":"Differentiable Causal Discovery For Latent Hierarchical Causal Models","abstract":"Discovering causal structures with latent variables from observational data\nis a fundamental challenge in causal discovery. Existing methods often rely on\nconstraint-based, iterative discrete searches, limiting their scalability to\nlarge numbers of variables. Moreover, these methods frequently assume linearity\nor invertibility, restricting their applicability to real-world scenarios. We\npresent new theoretical results on the identifiability of nonlinear latent\nhierarchical causal models, relaxing previous assumptions in literature about\nthe deterministic nature of latent variables and exogenous noise. Building on\nthese insights, we develop a novel differentiable causal discovery algorithm\nthat efficiently estimates the structure of such models. To the best of our\nknowledge, this is the first work to propose a differentiable causal discovery\nmethod for nonlinear latent hierarchical models. Our approach outperforms\nexisting methods in both accuracy and scalability. We demonstrate its practical\nutility by learning interpretable hierarchical latent structures from\nhigh-dimensional image data and demonstrate its effectiveness on downstream\ntasks.","authors":["Parjanya Prashant","Ignavier Ng","Kun Zhang","Biwei Huang"],"url":"http://arxiv.org/pdf/2411.19556v1","published":"2024-11-29"}
{"title":"Differentiable Causal Discovery For Latent Hierarchical Causal Models","abstract":"Discovering causal structures with latent variables from observational data\nis a fundamental challenge in causal discovery. Existing methods often rely on\nconstraint-based, iterative discrete searches, limiting their scalability to\nlarge numbers of variables. Moreover, these methods frequently assume linearity\nor invertibility, restricting their applicability to real-world scenarios. We\npresent new theoretical results on the identifiability of nonlinear latent\nhierarchical causal models, relaxing previous assumptions in literature about\nthe deterministic nature of latent variables and exogenous noise. Building on\nthese insights, we develop a novel differentiable causal discovery algorithm\nthat efficiently estimates the structure of such models. To the best of our\nknowledge, this is the first work to propose a differentiable causal discovery\nmethod for nonlinear latent hierarchical models. Our approach outperforms\nexisting methods in both accuracy and scalability. We demonstrate its practical\nutility by learning interpretable hierarchical latent structures from\nhigh-dimensional image data and demonstrate its effectiveness on downstream\ntasks.","authors":["Parjanya Prashant","Ignavier Ng","Kun Zhang","Biwei Huang"],"url":"http://arxiv.org/pdf/2411.19556v1","published":"2024-11-29"}
{"title":"LLMForecaster: Improving Seasonal Event Forecasts with Unstructured Textual Data","abstract":"Modern time-series forecasting models often fail to make full use of rich\nunstructured information about the time series themselves. This lack of proper\nconditioning can lead to obvious model failures; for example, models may be\nunaware of the details of a particular product, and hence fail to anticipate\nseasonal surges in customer demand in the lead up to major exogenous events\nlike holidays for clearly relevant products. To address this shortcoming, this\npaper introduces a novel forecast post-processor -- which we call LLMForecaster\n-- that fine-tunes large language models (LLMs) to incorporate unstructured\nsemantic and contextual information and historical data to improve the\nforecasts from an existing demand forecasting pipeline. In an industry-scale\nretail application, we demonstrate that our technique yields statistically\nsignificantly forecast improvements across several sets of products subject to\nholiday-driven demand surges.","authors":["Hanyu Zhang","Chuck Arvin","Dmitry Efimov","Michael W. Mahoney","Dominique Perrault-Joncas","Shankar Ramasubramanian","Andrew Gordon Wilson","Malcolm Wolff"],"url":"http://arxiv.org/pdf/2412.02525v1","published":"2024-12-03"}
{"title":"Modeling and Discovering Direct Causes for Predictive Models","abstract":"We introduce a causal modeling framework that captures the input-output\nbehavior of predictive models (e.g., machine learning models) by representing\nit using causal graphs. The framework enables us to define and identify\nfeatures that directly cause the predictions, which has broad implications for\ndata collection and model evaluation. We show two assumptions under which the\ndirect causes can be discovered from data, one of which further simplifies the\ndiscovery process. In addition to providing sound and complete algorithms, we\npropose an optimization technique based on an independence rule that can be\nintegrated with the algorithms to speed up the discovery process both\ntheoretically and empirically.","authors":["Yizuo Chen","Amit Bhatia"],"url":"http://arxiv.org/pdf/2412.02878v1","published":"2024-12-03"}
{"title":"Complexity of Vector-valued Prediction: From Linear Models to Stochastic Convex Optimization","abstract":"We study the problem of learning vector-valued linear predictors: these are\nprediction rules parameterized by a matrix that maps an $m$-dimensional feature\nvector to a $k$-dimensional target. We focus on the fundamental case with a\nconvex and Lipschitz loss function, and show several new theoretical results\nthat shed light on the complexity of this problem and its connection to related\nlearning models. First, we give a tight characterization of the sample\ncomplexity of Empirical Risk Minimization (ERM) in this setting, establishing\nthat $\\smash{\\widetilde{\\Omega}}(k/\\epsilon^2)$ examples are necessary for ERM\nto reach $\\epsilon$ excess (population) risk; this provides for an exponential\nimprovement over recent results by Magen and Shamir (2023) in terms of the\ndependence on the target dimension $k$, and matches a classical upper bound due\nto Maurer (2016). Second, we present a black-box conversion from general\n$d$-dimensional Stochastic Convex Optimization (SCO) to vector-valued linear\nprediction, showing that any SCO problem can be embedded as a prediction\nproblem with $k=\\Theta(d)$ outputs. These results portray the setting of\nvector-valued linear prediction as bridging between two extensively studied yet\ndisparate learning models: linear models (corresponds to $k=1$) and general\n$d$-dimensional SCO (with $k=\\Theta(d)$).","authors":["Matan Schliserman","Tomer Koren"],"url":"http://arxiv.org/pdf/2412.04274v1","published":"2024-12-05"}
{"title":"Auto-Regressive Moving Diffusion Models for Time Series Forecasting","abstract":"Time series forecasting (TSF) is essential in various domains, and recent\nadvancements in diffusion-based TSF models have shown considerable promise.\nHowever, these models typically adopt traditional diffusion patterns, treating\nTSF as a noise-based conditional generation task. This approach neglects the\ninherent continuous sequential nature of time series, leading to a fundamental\nmisalignment between diffusion mechanisms and the TSF objective, thereby\nseverely impairing performance. To bridge this misalignment, and inspired by\nthe classic Auto-Regressive Moving Average (ARMA) theory, which views time\nseries as continuous sequential progressions evolving from previous data\npoints, we propose a novel Auto-Regressive Moving Diffusion (ARMD) model to\nfirst achieve the continuous sequential diffusion-based TSF. Unlike previous\nmethods that start from white Gaussian noise, our model employs chain-based\ndiffusion with priors, accurately modeling the evolution of time series and\nleveraging intermediate state information to improve forecasting accuracy and\nstability. Specifically, our approach reinterprets the diffusion process by\nconsidering future series as the initial state and historical series as the\nfinal state, with intermediate series generated using a sliding-based technique\nduring the forward process. This design aligns the diffusion model's sampling\nprocedure with the forecasting objective, resulting in an unconditional,\ncontinuous sequential diffusion TSF model. Extensive experiments conducted on\nseven widely used datasets demonstrate that our model achieves state-of-the-art\nperformance, significantly outperforming existing diffusion-based TSF models.\nOur code is available on GitHub: https://github.com/daxin007/ARMD.","authors":["Jiaxin Gao","Qinglong Cao","Yuntian Chen"],"url":"http://arxiv.org/pdf/2412.09328v1","published":"2024-12-12"}
{"title":"A Comprehensive Forecasting Framework based on Multi-Stage Hierarchical Forecasting Reconciliation and Adjustment","abstract":"Ads demand forecasting for Walmart's ad products plays a critical role in\nenabling effective resource planning, allocation, and management of ads\nperformance. In this paper, we introduce a comprehensive demand forecasting\nsystem that tackles hierarchical time series forecasting in business settings.\nThough traditional hierarchical reconciliation methods ensure forecasting\ncoherence, they often trade off accuracy for coherence especially at lower\nlevels and fail to capture the seasonality unique to each time-series in the\nhierarchy. Thus, we propose a novel framework \"Multi-Stage Hierarchical\nForecasting Reconciliation and Adjustment (Multi-Stage HiFoReAd)\" to address\nthe challenges of preserving seasonality, ensuring coherence, and improving\naccuracy. Our system first utilizes diverse models, ensembled through Bayesian\nOptimization (BO), achieving base forecasts. The generated base forecasts are\nthen passed into the Multi-Stage HiFoReAd framework. The initial stage refines\nthe hierarchy using Top-Down forecasts and \"harmonic alignment.\" The second\nstage aligns the higher levels' forecasts using MinTrace algorithm, following\nwhich the last two levels undergo \"harmonic alignment\" and \"stratified\nscaling\", to eventually achieve accurate and coherent forecasts across the\nwhole hierarchy. Our experiments on Walmart's internal Ads-demand dataset and 3\nother public datasets, each with 4 hierarchical levels, demonstrate that the\naverage Absolute Percentage Error from the cross-validation sets improve from\n3% to 40% across levels against BO-ensemble of models (LGBM, MSTL+ETS, Prophet)\nas well as from 1.2% to 92.9% against State-Of-The-Art models. In addition, the\nforecasts at all hierarchical levels are proved to be coherent. The proposed\nframework has been deployed and leveraged by Walmart's ads, sales and\noperations teams to track future demands, make informed decisions and plan\nresources.","authors":["Zhengchao Yang","Mithun Ghosh","Anish Saha","Dong Xu","Konstantin Shmakov","Kuang-chih Lee"],"url":"http://arxiv.org/pdf/2412.14718v1","published":"2024-12-19"}
{"title":"Neural Conformal Control for Time Series Forecasting","abstract":"We introduce a neural network conformal prediction method for time series\nthat enhances adaptivity in non-stationary environments. Our approach acts as a\nneural controller designed to achieve desired target coverage, leveraging\nauxiliary multi-view data with neural network encoders in an end-to-end manner\nto further enhance adaptivity. Additionally, our model is designed to enhance\nthe consistency of prediction intervals in different quantiles by integrating\nmonotonicity constraints and leverages data from related tasks to boost\nfew-shot learning performance. Using real-world datasets from epidemics,\nelectric demand, weather, and others, we empirically demonstrate significant\nimprovements in coverage and probabilistic accuracy, and find that our method\nis the only one that combines good calibration with consistency in prediction\nintervals.","authors":["Ruipu Li","Alexander Rodr\u00edguez"],"url":"http://arxiv.org/pdf/2412.18144v1","published":"2024-12-24"}
{"title":"TimeRAF: Retrieval-Augmented Foundation model for Zero-shot Time Series Forecasting","abstract":"Time series forecasting plays a crucial role in data mining, driving rapid\nadvancements across numerous industries. With the emergence of large models,\ntime series foundation models (TSFMs) have exhibited remarkable generalization\ncapabilities, such as zero-shot learning, through large-scale pre-training.\nMeanwhile, Retrieval-Augmented Generation (RAG) methods have been widely\nemployed to enhance the performance of foundation models on unseen data,\nallowing models to access to external knowledge. In this paper, we introduce\nTimeRAF, a Retrieval-Augmented Forecasting model that enhance zero-shot time\nseries forecasting through retrieval-augmented techniques. We develop\ncustomized time series knowledge bases that are tailored to the specific\nforecasting tasks. TimeRAF employs an end-to-end learnable retriever to extract\nvaluable information from the knowledge base. Additionally, we propose Channel\nPrompting for knowledge integration, which effectively extracts relevant\ninformation from the retrieved knowledge along the channel dimension. Extensive\nexperiments demonstrate the effectiveness of our model, showing significant\nimprovement across various domains and datasets.","authors":["Huanyu Zhang","Chang Xu","Yi-Fan Zhang","Zhang Zhang","Liang Wang","Jiang Bian","Tieniu Tan"],"url":"http://arxiv.org/pdf/2412.20810v1","published":"2024-12-30"}
{"title":"Battling the Non-stationarity in Time Series Forecasting via Test-time Adaptation","abstract":"Deep Neural Networks have spearheaded remarkable advancements in time series\nforecasting (TSF), one of the major tasks in time series modeling. Nonetheless,\nthe non-stationarity of time series undermines the reliability of pre-trained\nsource time series forecasters in mission-critical deployment settings. In this\nstudy, we introduce a pioneering test-time adaptation framework tailored for\nTSF (TSF-TTA). TAFAS, the proposed approach to TSF-TTA, flexibly adapts source\nforecasters to continuously shifting test distributions while preserving the\ncore semantic information learned during pre-training. The novel utilization of\npartially-observed ground truth and gated calibration module enables proactive,\nrobust, and model-agnostic adaptation of source forecasters. Experiments on\ndiverse benchmark datasets and cutting-edge architectures demonstrate the\nefficacy and generality of TAFAS, especially in long-term forecasting scenarios\nthat suffer from significant distribution shifts. The code is available at\nhttps://github.com/kimanki/TAFAS.","authors":["HyunGi Kim","Siwon Kim","Jisoo Mok","Sungroh Yoon"],"url":"http://arxiv.org/pdf/2501.04970v1","published":"2025-01-09"}
{"title":"Kolmogorov-Arnold Networks for Time Series Granger Causality Inference","abstract":"We introduce Granger Causality Kolmogorov-Arnold Networks (GCKAN), an\ninnovative architecture that extends the recently proposed Kolmogorov-Arnold\nNetworks (KAN) to the domain of causal inference. By extracting base weights\nfrom KAN layers and incorporating the sparsity-inducing penalty along with\nridge regularization, GCKAN infers the Granger causality from time series while\nenabling automatic time lag selection. Additionally, we propose an algorithm\nleveraging time-reversed Granger causality to enhance inference accuracy. The\nalgorithm compares prediction and sparse-inducing losses derived from the\noriginal and time-reversed series, automatically selecting the casual\nrelationship with the higher score or integrating the results to mitigate\nspurious connectivities. Comprehensive experiments conducted on Lorenz-96, gene\nregulatory networks, fMRI BOLD signals, and VAR datasets demonstrate that the\nproposed model achieves competitive performance to state-of-the-art methods in\ninferring Granger causality from nonlinear, high-dimensional, and\nlimited-sample time series.","authors":["Meiliang Liu","Yunfang Xu","Zijin Li","Zhengye Si","Xiaoxiao Yang","Xinyue Yang","Zhiwen Zhao"],"url":"http://arxiv.org/pdf/2501.08958v1","published":"2025-01-15"}
{"title":"GCAD: Anomaly Detection in Multivariate Time Series from the Perspective of Granger Causality","abstract":"Multivariate time series anomaly detection has numerous real-world\napplications and is being extensively studied. Modeling pairwise correlations\nbetween variables is crucial. Existing methods employ learnable graph\nstructures and graph neural networks to explicitly model the spatial\ndependencies between variables. However, these methods are primarily based on\nprediction or reconstruction tasks, which can only learn similarity\nrelationships between sequence embeddings and lack interpretability in how\ngraph structures affect time series evolution. In this paper, we designed a\nframework that models spatial dependencies using interpretable causal\nrelationships and detects anomalies through changes in causal patterns.\nSpecifically, we propose a method to dynamically discover Granger causality\nusing gradients in nonlinear deep predictors and employ a simple sparsification\nstrategy to obtain a Granger causality graph, detecting anomalies from a causal\nperspective. Experiments on real-world datasets demonstrate that the proposed\nmodel achieves more accurate anomaly detection compared to baseline methods.","authors":["Zehao Liu","Mengzhou Gao","Pengfei Jiao"],"url":"http://arxiv.org/pdf/2501.13493v1","published":"2025-01-23"}
{"title":"Prediction-Powered Inference with Imputed Covariates and Nonuniform Sampling","abstract":"Machine learning models are increasingly used to produce predictions that\nserve as input data in subsequent statistical analyses. For example, computer\nvision predictions of economic and environmental indicators based on satellite\nimagery are used in downstream regressions; similarly, language models are\nwidely used to approximate human ratings and opinions in social science\nresearch. However, failure to properly account for errors in the machine\nlearning predictions renders standard statistical procedures invalid. Prior\nwork uses what we call the Predict-Then-Debias estimator to give valid\nconfidence intervals when machine learning algorithms impute missing variables,\nassuming a small complete sample from the population of interest. We expand the\nscope by introducing bootstrap confidence intervals that apply when the\ncomplete data is a nonuniform (i.e., weighted, stratified, or clustered) sample\nand to settings where an arbitrary subset of features is imputed. Importantly,\nthe method can be applied to many settings without requiring additional\ncalculations. We prove that these confidence intervals are valid under no\nassumptions on the quality of the machine learning model and are no wider than\nthe intervals obtained by methods that do not use machine learning predictions.","authors":["Dan M. Kluger","Kerri Lu","Tijana Zrnic","Sherrie Wang","Stephen Bates"],"url":"http://arxiv.org/pdf/2501.18577v1","published":"2025-01-30"}
{"title":"On the importance of structural identifiability for machine learning with partially observed dynamical systems","abstract":"The successful application of modern machine learning for time series\nclassification is often hampered by limitations in quality and quantity of\navailable training data. To overcome these limitations, available domain expert\nknowledge in the form of parametrised mechanistic dynamical models can be used\nwhenever it is available and time series observations may be represented as an\nelement from a given class of parametrised dynamical models. This makes the\nlearning process interpretable and allows the modeller to deal with sparsely\nand irregularly sampled data in a natural way. However, the internal processes\nof a dynamical model are often only partially observed. This can lead to\nambiguity regarding which particular model realization best explains a given\ntime series observation. This problem is well-known in the literature, and a\ndynamical model with this issue is referred to as structurally unidentifiable.\nTraining a classifier that incorporates knowledge about a structurally\nunidentifiable dynamical model can negatively influence classification\nperformance. To address this issue, we employ structural identifiability\nanalysis to explicitly relate parameter configurations that are associated with\nidentical system outputs. Using the derived relations in classifier training,\nwe demonstrate that this method significantly improves the classifier's ability\nto generalize to unseen data on a number of example models from the biomedical\ndomain. This effect is especially pronounced when the number of training\ninstances is limited. Our results demonstrate the importance of accounting for\nstructural identifiability, a topic that has received relatively little\nattention from the machine learning community.","authors":["Janis Norden","Elisa Oostwal","Michael Chappell","Peter Tino","Kerstin Bunte"],"url":"http://arxiv.org/pdf/2502.04131v1","published":"2025-02-06"}
{"title":"Relational Conformal Prediction for Correlated Time Series","abstract":"We address the problem of uncertainty quantification in time series\nforecasting by exploiting observations at correlated sequences. Relational deep\nlearning methods leveraging graph representations are among the most effective\ntools for obtaining point estimates from spatiotemporal data and correlated\ntime series. However, the problem of exploiting relational structures to\nestimate the uncertainty of such predictions has been largely overlooked in the\nsame context. To this end, we propose a novel distribution-free approach based\non the conformal prediction framework and quantile regression. Despite the\nrecent applications of conformal prediction to sequential data, existing\nmethods operate independently on each target time series and do not account for\nrelationships among them when constructing the prediction interval. We fill\nthis void by introducing a novel conformal prediction method based on graph\ndeep learning operators. Our method, named Conformal Relational Prediction\n(CoRel), does not require the relational structure (graph) to be known as a\nprior and can be applied on top of any pre-trained time series predictor.\nAdditionally, CoRel includes an adaptive component to handle non-exchangeable\ndata and changes in the input time series. Our approach provides accurate\ncoverage and archives state-of-the-art uncertainty quantification in relevant\nbenchmarks.","authors":["Andrea Cini","Alexander Jenkins","Danilo Mandic","Cesare Alippi","Filippo Maria Bianchi"],"url":"http://arxiv.org/pdf/2502.09443v1","published":"2025-02-13"}
{"title":"Not All Data are Good Labels: On the Self-supervised Labeling for Time Series Forecasting","abstract":"Time Series Forecasting (TSF) is a crucial task in various domains, yet\nexisting TSF models rely heavily on high-quality data and insufficiently\nexploit all available data. This paper explores a novel self-supervised\napproach to re-label time series datasets by inherently constructing candidate\ndatasets. During the optimization of a simple reconstruction network,\nintermediates are used as pseudo labels in a self-supervised paradigm,\nimproving generalization for any predictor. We introduce the Self-Correction\nwith Adaptive Mask (SCAM), which discards overfitted components and selectively\nreplaces them with pseudo labels generated from reconstructions. Additionally,\nwe incorporate Spectral Norm Regularization (SNR) to further suppress\noverfitting from a loss landscape perspective. Our experiments on eleven\nreal-world datasets demonstrate that SCAM consistently improves the performance\nof various backbone models. This work offers a new perspective on constructing\ndatasets and enhancing the generalization of TSF models through self-supervised\nlearning.","authors":["Yuxuan Yang","Dalin Zhang","Yuxuan Liang","Hua Lu","Huan Li","Gang Chen"],"url":"http://arxiv.org/pdf/2502.14704v1","published":"2025-02-20"}
