{"title":"DualCast: Disentangling Aperiodic Events from Traffic Series with a Dual-Branch Model","abstract":"Traffic forecasting is an important problem in the operation and optimisation\nof transportation systems. State-of-the-art solutions train machine learning\nmodels by minimising the mean forecasting errors on the training data. The\ntrained models often favour periodic events instead of aperiodic ones in their\nprediction results, as periodic events often prevail in the training data.\nWhile offering critical optimisation opportunities, aperiodic events such as\ntraffic incidents may be missed by the existing models. To address this issue,\nwe propose DualCast -- a model framework to enhance the learning capability of\ntraffic forecasting models, especially for aperiodic events. DualCast takes a\ndual-branch architecture, to disentangle traffic signals into two types, one\nreflecting intrinsic {spatial-temporal} patterns and the other reflecting\nexternal environment contexts including aperiodic events. We further propose a\ncross-time attention mechanism, to capture high-order spatial-temporal\nrelationships from both periodic and aperiodic patterns. DualCast is versatile.\nWe integrate it with recent traffic forecasting models, consistently reducing\ntheir forecasting errors by up to 9.6% on multiple real datasets.","authors":["Xinyu Su","Feng Liu","Yanchuan Chang","Egemen Tanin","Majid Sarvi","Jianzhong Qi"],"url":"http://arxiv.org/pdf/2411.18286v1","published":"2024-11-27"}
{"title":"DualCast: Disentangling Aperiodic Events from Traffic Series with a Dual-Branch Model","abstract":"Traffic forecasting is an important problem in the operation and optimisation\nof transportation systems. State-of-the-art solutions train machine learning\nmodels by minimising the mean forecasting errors on the training data. The\ntrained models often favour periodic events instead of aperiodic ones in their\nprediction results, as periodic events often prevail in the training data.\nWhile offering critical optimisation opportunities, aperiodic events such as\ntraffic incidents may be missed by the existing models. To address this issue,\nwe propose DualCast -- a model framework to enhance the learning capability of\ntraffic forecasting models, especially for aperiodic events. DualCast takes a\ndual-branch architecture, to disentangle traffic signals into two types, one\nreflecting intrinsic {spatial-temporal} patterns and the other reflecting\nexternal environment contexts including aperiodic events. We further propose a\ncross-time attention mechanism, to capture high-order spatial-temporal\nrelationships from both periodic and aperiodic patterns. DualCast is versatile.\nWe integrate it with recent traffic forecasting models, consistently reducing\ntheir forecasting errors by up to 9.6% on multiple real datasets.","authors":["Xinyu Su","Feng Liu","Yanchuan Chang","Egemen Tanin","Majid Sarvi","Jianzhong Qi"],"url":"http://arxiv.org/pdf/2411.18286v1","published":"2024-11-27"}
{"title":"DualCast: Disentangling Aperiodic Events from Traffic Series with a Dual-Branch Model","abstract":"Traffic forecasting is an important problem in the operation and optimisation\nof transportation systems. State-of-the-art solutions train machine learning\nmodels by minimising the mean forecasting errors on the training data. The\ntrained models often favour periodic events instead of aperiodic ones in their\nprediction results, as periodic events often prevail in the training data.\nWhile offering critical optimisation opportunities, aperiodic events such as\ntraffic incidents may be missed by the existing models. To address this issue,\nwe propose DualCast -- a model framework to enhance the learning capability of\ntraffic forecasting models, especially for aperiodic events. DualCast takes a\ndual-branch architecture, to disentangle traffic signals into two types, one\nreflecting intrinsic {spatial-temporal} patterns and the other reflecting\nexternal environment contexts including aperiodic events. We further propose a\ncross-time attention mechanism, to capture high-order spatial-temporal\nrelationships from both periodic and aperiodic patterns. DualCast is versatile.\nWe integrate it with recent traffic forecasting models, consistently reducing\ntheir forecasting errors by up to 9.6% on multiple real datasets.","authors":["Xinyu Su","Feng Liu","Yanchuan Chang","Egemen Tanin","Majid Sarvi","Jianzhong Qi"],"url":"http://arxiv.org/pdf/2411.18286v1","published":"2024-11-27"}
{"title":"DualCast: Disentangling Aperiodic Events from Traffic Series with a Dual-Branch Model","abstract":"Traffic forecasting is an important problem in the operation and optimisation\nof transportation systems. State-of-the-art solutions train machine learning\nmodels by minimising the mean forecasting errors on the training data. The\ntrained models often favour periodic events instead of aperiodic ones in their\nprediction results, as periodic events often prevail in the training data.\nWhile offering critical optimisation opportunities, aperiodic events such as\ntraffic incidents may be missed by the existing models. To address this issue,\nwe propose DualCast -- a model framework to enhance the learning capability of\ntraffic forecasting models, especially for aperiodic events. DualCast takes a\ndual-branch architecture, to disentangle traffic signals into two types, one\nreflecting intrinsic {spatial-temporal} patterns and the other reflecting\nexternal environment contexts including aperiodic events. We further propose a\ncross-time attention mechanism, to capture high-order spatial-temporal\nrelationships from both periodic and aperiodic patterns. DualCast is versatile.\nWe integrate it with recent traffic forecasting models, consistently reducing\ntheir forecasting errors by up to 9.6% on multiple real datasets.","authors":["Xinyu Su","Feng Liu","Yanchuan Chang","Egemen Tanin","Majid Sarvi","Jianzhong Qi"],"url":"http://arxiv.org/pdf/2411.18286v1","published":"2024-11-27"}
{"title":"Differentiable Causal Discovery For Latent Hierarchical Causal Models","abstract":"Discovering causal structures with latent variables from observational data\nis a fundamental challenge in causal discovery. Existing methods often rely on\nconstraint-based, iterative discrete searches, limiting their scalability to\nlarge numbers of variables. Moreover, these methods frequently assume linearity\nor invertibility, restricting their applicability to real-world scenarios. We\npresent new theoretical results on the identifiability of nonlinear latent\nhierarchical causal models, relaxing previous assumptions in literature about\nthe deterministic nature of latent variables and exogenous noise. Building on\nthese insights, we develop a novel differentiable causal discovery algorithm\nthat efficiently estimates the structure of such models. To the best of our\nknowledge, this is the first work to propose a differentiable causal discovery\nmethod for nonlinear latent hierarchical models. Our approach outperforms\nexisting methods in both accuracy and scalability. We demonstrate its practical\nutility by learning interpretable hierarchical latent structures from\nhigh-dimensional image data and demonstrate its effectiveness on downstream\ntasks.","authors":["Parjanya Prashant","Ignavier Ng","Kun Zhang","Biwei Huang"],"url":"http://arxiv.org/pdf/2411.19556v1","published":"2024-11-29"}
{"title":"Differentiable Causal Discovery For Latent Hierarchical Causal Models","abstract":"Discovering causal structures with latent variables from observational data\nis a fundamental challenge in causal discovery. Existing methods often rely on\nconstraint-based, iterative discrete searches, limiting their scalability to\nlarge numbers of variables. Moreover, these methods frequently assume linearity\nor invertibility, restricting their applicability to real-world scenarios. We\npresent new theoretical results on the identifiability of nonlinear latent\nhierarchical causal models, relaxing previous assumptions in literature about\nthe deterministic nature of latent variables and exogenous noise. Building on\nthese insights, we develop a novel differentiable causal discovery algorithm\nthat efficiently estimates the structure of such models. To the best of our\nknowledge, this is the first work to propose a differentiable causal discovery\nmethod for nonlinear latent hierarchical models. Our approach outperforms\nexisting methods in both accuracy and scalability. We demonstrate its practical\nutility by learning interpretable hierarchical latent structures from\nhigh-dimensional image data and demonstrate its effectiveness on downstream\ntasks.","authors":["Parjanya Prashant","Ignavier Ng","Kun Zhang","Biwei Huang"],"url":"http://arxiv.org/pdf/2411.19556v1","published":"2024-11-29"}
{"title":"LLMForecaster: Improving Seasonal Event Forecasts with Unstructured Textual Data","abstract":"Modern time-series forecasting models often fail to make full use of rich\nunstructured information about the time series themselves. This lack of proper\nconditioning can lead to obvious model failures; for example, models may be\nunaware of the details of a particular product, and hence fail to anticipate\nseasonal surges in customer demand in the lead up to major exogenous events\nlike holidays for clearly relevant products. To address this shortcoming, this\npaper introduces a novel forecast post-processor -- which we call LLMForecaster\n-- that fine-tunes large language models (LLMs) to incorporate unstructured\nsemantic and contextual information and historical data to improve the\nforecasts from an existing demand forecasting pipeline. In an industry-scale\nretail application, we demonstrate that our technique yields statistically\nsignificantly forecast improvements across several sets of products subject to\nholiday-driven demand surges.","authors":["Hanyu Zhang","Chuck Arvin","Dmitry Efimov","Michael W. Mahoney","Dominique Perrault-Joncas","Shankar Ramasubramanian","Andrew Gordon Wilson","Malcolm Wolff"],"url":"http://arxiv.org/pdf/2412.02525v1","published":"2024-12-03"}
{"title":"Modeling and Discovering Direct Causes for Predictive Models","abstract":"We introduce a causal modeling framework that captures the input-output\nbehavior of predictive models (e.g., machine learning models) by representing\nit using causal graphs. The framework enables us to define and identify\nfeatures that directly cause the predictions, which has broad implications for\ndata collection and model evaluation. We show two assumptions under which the\ndirect causes can be discovered from data, one of which further simplifies the\ndiscovery process. In addition to providing sound and complete algorithms, we\npropose an optimization technique based on an independence rule that can be\nintegrated with the algorithms to speed up the discovery process both\ntheoretically and empirically.","authors":["Yizuo Chen","Amit Bhatia"],"url":"http://arxiv.org/pdf/2412.02878v1","published":"2024-12-03"}
{"title":"Complexity of Vector-valued Prediction: From Linear Models to Stochastic Convex Optimization","abstract":"We study the problem of learning vector-valued linear predictors: these are\nprediction rules parameterized by a matrix that maps an $m$-dimensional feature\nvector to a $k$-dimensional target. We focus on the fundamental case with a\nconvex and Lipschitz loss function, and show several new theoretical results\nthat shed light on the complexity of this problem and its connection to related\nlearning models. First, we give a tight characterization of the sample\ncomplexity of Empirical Risk Minimization (ERM) in this setting, establishing\nthat $\\smash{\\widetilde{\\Omega}}(k/\\epsilon^2)$ examples are necessary for ERM\nto reach $\\epsilon$ excess (population) risk; this provides for an exponential\nimprovement over recent results by Magen and Shamir (2023) in terms of the\ndependence on the target dimension $k$, and matches a classical upper bound due\nto Maurer (2016). Second, we present a black-box conversion from general\n$d$-dimensional Stochastic Convex Optimization (SCO) to vector-valued linear\nprediction, showing that any SCO problem can be embedded as a prediction\nproblem with $k=\\Theta(d)$ outputs. These results portray the setting of\nvector-valued linear prediction as bridging between two extensively studied yet\ndisparate learning models: linear models (corresponds to $k=1$) and general\n$d$-dimensional SCO (with $k=\\Theta(d)$).","authors":["Matan Schliserman","Tomer Koren"],"url":"http://arxiv.org/pdf/2412.04274v1","published":"2024-12-05"}
{"title":"Auto-Regressive Moving Diffusion Models for Time Series Forecasting","abstract":"Time series forecasting (TSF) is essential in various domains, and recent\nadvancements in diffusion-based TSF models have shown considerable promise.\nHowever, these models typically adopt traditional diffusion patterns, treating\nTSF as a noise-based conditional generation task. This approach neglects the\ninherent continuous sequential nature of time series, leading to a fundamental\nmisalignment between diffusion mechanisms and the TSF objective, thereby\nseverely impairing performance. To bridge this misalignment, and inspired by\nthe classic Auto-Regressive Moving Average (ARMA) theory, which views time\nseries as continuous sequential progressions evolving from previous data\npoints, we propose a novel Auto-Regressive Moving Diffusion (ARMD) model to\nfirst achieve the continuous sequential diffusion-based TSF. Unlike previous\nmethods that start from white Gaussian noise, our model employs chain-based\ndiffusion with priors, accurately modeling the evolution of time series and\nleveraging intermediate state information to improve forecasting accuracy and\nstability. Specifically, our approach reinterprets the diffusion process by\nconsidering future series as the initial state and historical series as the\nfinal state, with intermediate series generated using a sliding-based technique\nduring the forward process. This design aligns the diffusion model's sampling\nprocedure with the forecasting objective, resulting in an unconditional,\ncontinuous sequential diffusion TSF model. Extensive experiments conducted on\nseven widely used datasets demonstrate that our model achieves state-of-the-art\nperformance, significantly outperforming existing diffusion-based TSF models.\nOur code is available on GitHub: https://github.com/daxin007/ARMD.","authors":["Jiaxin Gao","Qinglong Cao","Yuntian Chen"],"url":"http://arxiv.org/pdf/2412.09328v1","published":"2024-12-12"}
{"title":"A Comprehensive Forecasting Framework based on Multi-Stage Hierarchical Forecasting Reconciliation and Adjustment","abstract":"Ads demand forecasting for Walmart's ad products plays a critical role in\nenabling effective resource planning, allocation, and management of ads\nperformance. In this paper, we introduce a comprehensive demand forecasting\nsystem that tackles hierarchical time series forecasting in business settings.\nThough traditional hierarchical reconciliation methods ensure forecasting\ncoherence, they often trade off accuracy for coherence especially at lower\nlevels and fail to capture the seasonality unique to each time-series in the\nhierarchy. Thus, we propose a novel framework \"Multi-Stage Hierarchical\nForecasting Reconciliation and Adjustment (Multi-Stage HiFoReAd)\" to address\nthe challenges of preserving seasonality, ensuring coherence, and improving\naccuracy. Our system first utilizes diverse models, ensembled through Bayesian\nOptimization (BO), achieving base forecasts. The generated base forecasts are\nthen passed into the Multi-Stage HiFoReAd framework. The initial stage refines\nthe hierarchy using Top-Down forecasts and \"harmonic alignment.\" The second\nstage aligns the higher levels' forecasts using MinTrace algorithm, following\nwhich the last two levels undergo \"harmonic alignment\" and \"stratified\nscaling\", to eventually achieve accurate and coherent forecasts across the\nwhole hierarchy. Our experiments on Walmart's internal Ads-demand dataset and 3\nother public datasets, each with 4 hierarchical levels, demonstrate that the\naverage Absolute Percentage Error from the cross-validation sets improve from\n3% to 40% across levels against BO-ensemble of models (LGBM, MSTL+ETS, Prophet)\nas well as from 1.2% to 92.9% against State-Of-The-Art models. In addition, the\nforecasts at all hierarchical levels are proved to be coherent. The proposed\nframework has been deployed and leveraged by Walmart's ads, sales and\noperations teams to track future demands, make informed decisions and plan\nresources.","authors":["Zhengchao Yang","Mithun Ghosh","Anish Saha","Dong Xu","Konstantin Shmakov","Kuang-chih Lee"],"url":"http://arxiv.org/pdf/2412.14718v1","published":"2024-12-19"}
{"title":"Neural Conformal Control for Time Series Forecasting","abstract":"We introduce a neural network conformal prediction method for time series\nthat enhances adaptivity in non-stationary environments. Our approach acts as a\nneural controller designed to achieve desired target coverage, leveraging\nauxiliary multi-view data with neural network encoders in an end-to-end manner\nto further enhance adaptivity. Additionally, our model is designed to enhance\nthe consistency of prediction intervals in different quantiles by integrating\nmonotonicity constraints and leverages data from related tasks to boost\nfew-shot learning performance. Using real-world datasets from epidemics,\nelectric demand, weather, and others, we empirically demonstrate significant\nimprovements in coverage and probabilistic accuracy, and find that our method\nis the only one that combines good calibration with consistency in prediction\nintervals.","authors":["Ruipu Li","Alexander Rodr\u00edguez"],"url":"http://arxiv.org/pdf/2412.18144v1","published":"2024-12-24"}
{"title":"TimeRAF: Retrieval-Augmented Foundation model for Zero-shot Time Series Forecasting","abstract":"Time series forecasting plays a crucial role in data mining, driving rapid\nadvancements across numerous industries. With the emergence of large models,\ntime series foundation models (TSFMs) have exhibited remarkable generalization\ncapabilities, such as zero-shot learning, through large-scale pre-training.\nMeanwhile, Retrieval-Augmented Generation (RAG) methods have been widely\nemployed to enhance the performance of foundation models on unseen data,\nallowing models to access to external knowledge. In this paper, we introduce\nTimeRAF, a Retrieval-Augmented Forecasting model that enhance zero-shot time\nseries forecasting through retrieval-augmented techniques. We develop\ncustomized time series knowledge bases that are tailored to the specific\nforecasting tasks. TimeRAF employs an end-to-end learnable retriever to extract\nvaluable information from the knowledge base. Additionally, we propose Channel\nPrompting for knowledge integration, which effectively extracts relevant\ninformation from the retrieved knowledge along the channel dimension. Extensive\nexperiments demonstrate the effectiveness of our model, showing significant\nimprovement across various domains and datasets.","authors":["Huanyu Zhang","Chang Xu","Yi-Fan Zhang","Zhang Zhang","Liang Wang","Jiang Bian","Tieniu Tan"],"url":"http://arxiv.org/pdf/2412.20810v1","published":"2024-12-30"}
{"title":"Battling the Non-stationarity in Time Series Forecasting via Test-time Adaptation","abstract":"Deep Neural Networks have spearheaded remarkable advancements in time series\nforecasting (TSF), one of the major tasks in time series modeling. Nonetheless,\nthe non-stationarity of time series undermines the reliability of pre-trained\nsource time series forecasters in mission-critical deployment settings. In this\nstudy, we introduce a pioneering test-time adaptation framework tailored for\nTSF (TSF-TTA). TAFAS, the proposed approach to TSF-TTA, flexibly adapts source\nforecasters to continuously shifting test distributions while preserving the\ncore semantic information learned during pre-training. The novel utilization of\npartially-observed ground truth and gated calibration module enables proactive,\nrobust, and model-agnostic adaptation of source forecasters. Experiments on\ndiverse benchmark datasets and cutting-edge architectures demonstrate the\nefficacy and generality of TAFAS, especially in long-term forecasting scenarios\nthat suffer from significant distribution shifts. The code is available at\nhttps://github.com/kimanki/TAFAS.","authors":["HyunGi Kim","Siwon Kim","Jisoo Mok","Sungroh Yoon"],"url":"http://arxiv.org/pdf/2501.04970v1","published":"2025-01-09"}
{"title":"Kolmogorov-Arnold Networks for Time Series Granger Causality Inference","abstract":"We introduce Granger Causality Kolmogorov-Arnold Networks (GCKAN), an\ninnovative architecture that extends the recently proposed Kolmogorov-Arnold\nNetworks (KAN) to the domain of causal inference. By extracting base weights\nfrom KAN layers and incorporating the sparsity-inducing penalty along with\nridge regularization, GCKAN infers the Granger causality from time series while\nenabling automatic time lag selection. Additionally, we propose an algorithm\nleveraging time-reversed Granger causality to enhance inference accuracy. The\nalgorithm compares prediction and sparse-inducing losses derived from the\noriginal and time-reversed series, automatically selecting the casual\nrelationship with the higher score or integrating the results to mitigate\nspurious connectivities. Comprehensive experiments conducted on Lorenz-96, gene\nregulatory networks, fMRI BOLD signals, and VAR datasets demonstrate that the\nproposed model achieves competitive performance to state-of-the-art methods in\ninferring Granger causality from nonlinear, high-dimensional, and\nlimited-sample time series.","authors":["Meiliang Liu","Yunfang Xu","Zijin Li","Zhengye Si","Xiaoxiao Yang","Xinyue Yang","Zhiwen Zhao"],"url":"http://arxiv.org/pdf/2501.08958v1","published":"2025-01-15"}
{"title":"GCAD: Anomaly Detection in Multivariate Time Series from the Perspective of Granger Causality","abstract":"Multivariate time series anomaly detection has numerous real-world\napplications and is being extensively studied. Modeling pairwise correlations\nbetween variables is crucial. Existing methods employ learnable graph\nstructures and graph neural networks to explicitly model the spatial\ndependencies between variables. However, these methods are primarily based on\nprediction or reconstruction tasks, which can only learn similarity\nrelationships between sequence embeddings and lack interpretability in how\ngraph structures affect time series evolution. In this paper, we designed a\nframework that models spatial dependencies using interpretable causal\nrelationships and detects anomalies through changes in causal patterns.\nSpecifically, we propose a method to dynamically discover Granger causality\nusing gradients in nonlinear deep predictors and employ a simple sparsification\nstrategy to obtain a Granger causality graph, detecting anomalies from a causal\nperspective. Experiments on real-world datasets demonstrate that the proposed\nmodel achieves more accurate anomaly detection compared to baseline methods.","authors":["Zehao Liu","Mengzhou Gao","Pengfei Jiao"],"url":"http://arxiv.org/pdf/2501.13493v1","published":"2025-01-23"}
{"title":"Prediction-Powered Inference with Imputed Covariates and Nonuniform Sampling","abstract":"Machine learning models are increasingly used to produce predictions that\nserve as input data in subsequent statistical analyses. For example, computer\nvision predictions of economic and environmental indicators based on satellite\nimagery are used in downstream regressions; similarly, language models are\nwidely used to approximate human ratings and opinions in social science\nresearch. However, failure to properly account for errors in the machine\nlearning predictions renders standard statistical procedures invalid. Prior\nwork uses what we call the Predict-Then-Debias estimator to give valid\nconfidence intervals when machine learning algorithms impute missing variables,\nassuming a small complete sample from the population of interest. We expand the\nscope by introducing bootstrap confidence intervals that apply when the\ncomplete data is a nonuniform (i.e., weighted, stratified, or clustered) sample\nand to settings where an arbitrary subset of features is imputed. Importantly,\nthe method can be applied to many settings without requiring additional\ncalculations. We prove that these confidence intervals are valid under no\nassumptions on the quality of the machine learning model and are no wider than\nthe intervals obtained by methods that do not use machine learning predictions.","authors":["Dan M. Kluger","Kerri Lu","Tijana Zrnic","Sherrie Wang","Stephen Bates"],"url":"http://arxiv.org/pdf/2501.18577v1","published":"2025-01-30"}
{"title":"On the importance of structural identifiability for machine learning with partially observed dynamical systems","abstract":"The successful application of modern machine learning for time series\nclassification is often hampered by limitations in quality and quantity of\navailable training data. To overcome these limitations, available domain expert\nknowledge in the form of parametrised mechanistic dynamical models can be used\nwhenever it is available and time series observations may be represented as an\nelement from a given class of parametrised dynamical models. This makes the\nlearning process interpretable and allows the modeller to deal with sparsely\nand irregularly sampled data in a natural way. However, the internal processes\nof a dynamical model are often only partially observed. This can lead to\nambiguity regarding which particular model realization best explains a given\ntime series observation. This problem is well-known in the literature, and a\ndynamical model with this issue is referred to as structurally unidentifiable.\nTraining a classifier that incorporates knowledge about a structurally\nunidentifiable dynamical model can negatively influence classification\nperformance. To address this issue, we employ structural identifiability\nanalysis to explicitly relate parameter configurations that are associated with\nidentical system outputs. Using the derived relations in classifier training,\nwe demonstrate that this method significantly improves the classifier's ability\nto generalize to unseen data on a number of example models from the biomedical\ndomain. This effect is especially pronounced when the number of training\ninstances is limited. Our results demonstrate the importance of accounting for\nstructural identifiability, a topic that has received relatively little\nattention from the machine learning community.","authors":["Janis Norden","Elisa Oostwal","Michael Chappell","Peter Tino","Kerstin Bunte"],"url":"http://arxiv.org/pdf/2502.04131v1","published":"2025-02-06"}
{"title":"Relational Conformal Prediction for Correlated Time Series","abstract":"We address the problem of uncertainty quantification in time series\nforecasting by exploiting observations at correlated sequences. Relational deep\nlearning methods leveraging graph representations are among the most effective\ntools for obtaining point estimates from spatiotemporal data and correlated\ntime series. However, the problem of exploiting relational structures to\nestimate the uncertainty of such predictions has been largely overlooked in the\nsame context. To this end, we propose a novel distribution-free approach based\non the conformal prediction framework and quantile regression. Despite the\nrecent applications of conformal prediction to sequential data, existing\nmethods operate independently on each target time series and do not account for\nrelationships among them when constructing the prediction interval. We fill\nthis void by introducing a novel conformal prediction method based on graph\ndeep learning operators. Our method, named Conformal Relational Prediction\n(CoRel), does not require the relational structure (graph) to be known as a\nprior and can be applied on top of any pre-trained time series predictor.\nAdditionally, CoRel includes an adaptive component to handle non-exchangeable\ndata and changes in the input time series. Our approach provides accurate\ncoverage and archives state-of-the-art uncertainty quantification in relevant\nbenchmarks.","authors":["Andrea Cini","Alexander Jenkins","Danilo Mandic","Cesare Alippi","Filippo Maria Bianchi"],"url":"http://arxiv.org/pdf/2502.09443v1","published":"2025-02-13"}
{"title":"Not All Data are Good Labels: On the Self-supervised Labeling for Time Series Forecasting","abstract":"Time Series Forecasting (TSF) is a crucial task in various domains, yet\nexisting TSF models rely heavily on high-quality data and insufficiently\nexploit all available data. This paper explores a novel self-supervised\napproach to re-label time series datasets by inherently constructing candidate\ndatasets. During the optimization of a simple reconstruction network,\nintermediates are used as pseudo labels in a self-supervised paradigm,\nimproving generalization for any predictor. We introduce the Self-Correction\nwith Adaptive Mask (SCAM), which discards overfitted components and selectively\nreplaces them with pseudo labels generated from reconstructions. Additionally,\nwe incorporate Spectral Norm Regularization (SNR) to further suppress\noverfitting from a loss landscape perspective. Our experiments on eleven\nreal-world datasets demonstrate that SCAM consistently improves the performance\nof various backbone models. This work offers a new perspective on constructing\ndatasets and enhancing the generalization of TSF models through self-supervised\nlearning.","authors":["Yuxuan Yang","Dalin Zhang","Yuxuan Liang","Hua Lu","Huan Li","Gang Chen"],"url":"http://arxiv.org/pdf/2502.14704v1","published":"2025-02-20"}
{"title":"Efficient Time Series Forecasting via Hyper-Complex Models and Frequency Aggregation","abstract":"Time series forecasting is a long-standing problem in statistics and machine\nlearning. One of the key challenges is processing sequences with long-range\ndependencies. To that end, a recent line of work applied the short-time Fourier\ntransform (STFT), which partitions the sequence into multiple subsequences and\napplies a Fourier transform to each separately. We propose the Frequency\nInformation Aggregation (FIA)-Net, which is based on a novel complex-valued MLP\narchitecture that aggregates adjacent window information in the frequency\ndomain. To further increase the receptive field of the FIA-Net, we treat the\nset of windows as hyper-complex (HC) valued vectors and employ HC algebra to\nefficiently combine information from all STFT windows altogether. Using the\nHC-MLP backbone allows for improved handling of sequences with long-term\ndependence. Furthermore, due to the nature of HC operations, the HC-MLP uses up\nto three times fewer parameters than the equivalent standard window aggregation\nmethod. We evaluate the FIA-Net on various time-series benchmarks and show that\nthe proposed methodologies outperform existing state of the art methods in\nterms of both accuracy and efficiency. Our code is publicly available on\nhttps://anonymous.4open.science/r/research-1803/.","authors":["Eyal Yakir","Dor Tsur","Haim Permuter"],"url":"http://arxiv.org/pdf/2502.19983v1","published":"2025-02-27"}
{"title":"TimeFound: A Foundation Model for Time Series Forecasting","abstract":"We present TimeFound, an encoder-decoder transformer-based time series\nfoundation model for out-of-the-box zero-shot forecasting. To handle time\nseries data from various domains, TimeFound employs a multi-resolution patching\nstrategy to capture complex temporal patterns at multiple scales. We pre-train\nour model with two sizes (200M and 710M parameters) on a large time-series\ncorpus comprising both real-world and synthetic datasets. Over a collection of\nunseen datasets across diverse domains and forecasting horizons, our empirical\nevaluations suggest that TimeFound can achieve superior or competitive\nzero-shot forecasting performance, compared to state-of-the-art time series\nfoundation models.","authors":["Congxi Xiao","Jingbo Zhou","Yixiong Xiao","Xinjiang Lu","Le Zhang","Hui Xiong"],"url":"http://arxiv.org/pdf/2503.04118v1","published":"2025-03-06"}
{"title":"Deep Learning for Time Series Forecasting: A Survey","abstract":"Time series forecasting (TSF) has long been a crucial task in both industry\nand daily life. Most classical statistical models may have certain limitations\nwhen applied to practical scenarios in fields such as energy, healthcare,\ntraffic, meteorology, and economics, especially when high accuracy is required.\nWith the continuous development of deep learning, numerous new models have\nemerged in the field of time series forecasting in recent years. However,\nexisting surveys have not provided a unified summary of the wide range of model\narchitectures in this field, nor have they given detailed summaries of works in\nfeature extraction and datasets. To address this gap, in this review, we\ncomprehensively study the previous works and summarize the general paradigms of\nDeep Time Series Forecasting (DTSF) in terms of model architectures. Besides,\nwe take an innovative approach by focusing on the composition of time series\nand systematically explain important feature extraction methods. Additionally,\nwe provide an overall compilation of datasets from various domains in existing\nworks. Finally, we systematically emphasize the significant challenges faced\nand future research directions in this field.","authors":["Xiangjie Kong","Zhenghao Chen","Weiyao Liu","Kaili Ning","Lechao Zhang","Syauqie Muhammad Marier","Yichen Liu","Yuhao Chen","Feng Xia"],"url":"http://arxiv.org/pdf/2503.10198v1","published":"2025-03-13"}
{"title":"Truthful Elicitation of Imprecise Forecasts","abstract":"The quality of probabilistic forecasts is crucial for decision-making under\nuncertainty. While proper scoring rules incentivize truthful reporting of\nprecise forecasts, they fall short when forecasters face epistemic uncertainty\nabout their beliefs, limiting their use in safety-critical domains where\ndecision-makers (DMs) prioritize proper uncertainty management. To address\nthis, we propose a framework for scoring imprecise forecasts -- forecasts given\nas a set of beliefs. Despite existing impossibility results for deterministic\nscoring rules, we enable truthful elicitation by drawing connection to social\nchoice theory and introducing a two-way communication framework where DMs first\nshare their aggregation rules (e.g., averaging or min-max) used in downstream\ndecisions for resolving forecast ambiguity. This, in turn, helps forecasters\nresolve indecision during elicitation. We further show that truthful\nelicitation of imprecise forecasts is achievable using proper scoring rules\nrandomized over the aggregation procedure. Our approach allows DM to elicit and\nintegrate the forecaster's epistemic uncertainty into their decision-making\nprocess, thus improving credibility.","authors":["Anurag Singh","Siu Lun Chau","Krikamol Muandet"],"url":"http://arxiv.org/pdf/2503.16395v1","published":"2025-03-20"}
{"title":"Interpretable Cross-Sphere Multiscale Deep Learning Predicts ENSO Skilfully Beyond 2 Years","abstract":"El Ni\\~no-Southern Oscillation (ENSO) exerts global climate and societal\nimpacts, but real-time prediction with lead times beyond one year remains\nchallenging. Dynamical models suffer from large biases and uncertainties, while\ndeep learning struggles with interpretability and multi-scale dynamics. Here,\nwe introduce PTSTnet, an interpretable model that unifies dynamical processes\nand cross-scale spatiotemporal learning in an innovative neural-network\nframework with physics-encoding learning. PTSTnet produces interpretable\npredictions significantly outperforming state-of-the-art benchmarks with lead\ntimes beyond 24 months, providing physical insights into error propagation in\nocean-atmosphere interactions. PTSTnet learns feature representations with\nphysical consistency from sparse data to tackle inherent multi-scale and\nmulti-physics challenges underlying ocean-atmosphere processes, thereby\ninherently enhancing long-term prediction skill. Our successful realizations\nmark substantial steps forward in interpretable insights into innovative neural\nocean modelling.","authors":["Rixu Hao","Yuxin Zhao","Shaoqing Zhang","Guihua Wang","Xiong Deng"],"url":"http://arxiv.org/pdf/2503.21211v1","published":"2025-03-27"}
{"title":"Online Multivariate Regularized Distributional Regression for High-dimensional Probabilistic Electricity Price Forecasting","abstract":"Probabilistic electricity price forecasting (PEPF) is a key task for market\nparticipants in short-term electricity markets. The increasing availability of\nhigh-frequency data and the need for real-time decision-making in energy\nmarkets require online estimation methods for efficient model updating. We\npresent an online, multivariate, regularized distributional regression model,\nallowing for the modeling of all distribution parameters conditional on\nexplanatory variables. Our approach is based on the combination of the\nmultivariate distributional regression and an efficient online learning\nalgorithm based on online coordinate descent for LASSO-type regularization.\nAdditionally, we propose to regularize the estimation along a path of\nincreasingly complex dependence structures of the multivariate distribution,\nallowing for parsimonious estimation and early stopping. We validate our\napproach through one of the first forecasting studies focusing on multivariate\nprobabilistic forecasting in the German day-ahead electricity market while\nusing only online estimation methods. We compare our approach to online\nLASSO-ARX-models with adaptive marginal distribution and to online univariate\ndistributional models combined with an adaptive Copula. We show that the\nmultivariate distributional regression, which allows modeling all distribution\nparameters - including the mean and the dependence structure - conditional on\nexplanatory variables such as renewable in-feed or past prices provide superior\nforecasting performance compared to modeling of the marginals only and keeping\na static/unconditional dependence structure. Additionally, online estimation\nyields a speed-up by a factor of 80 to over 400 times compared to batch\nfitting.","authors":["Simon Hirsch"],"url":"http://arxiv.org/pdf/2504.02518v1","published":"2025-04-03"}
{"title":"Deep Learning Meets Teleconnections: Improving S2S Predictions for European Winter Weather","abstract":"Predictions on subseasonal-to-seasonal (S2S) timescales--ranging from two\nweeks to two month--are crucial for early warning systems but remain\nchallenging owing to chaos in the climate system. Teleconnections, such as the\nstratospheric polar vortex (SPV) and Madden-Julian Oscillation (MJO), offer\nwindows of enhanced predictability, however, their complex interactions remain\nunderutilized in operational forecasting. Here, we developed and evaluated deep\nlearning architectures to predict North Atlantic-European (NAE) weather\nregimes, systematically assessing the role of remote drivers in improving S2S\nforecast skill of deep learning models. We implemented (1) a Long Short-term\nMemory (LSTM) network predicting the NAE regimes of the next six weeks based on\nprevious regimes, (2) an Index-LSTM incorporating SPV and MJO indices, and (3)\na ViT-LSTM using a Vision Transformer to directly encode stratospheric wind and\ntropical outgoing longwave radiation fields. These models are compared with\noperational hindcasts as well as other AI models. Our results show that\nleveraging teleconnection information enhances skill at longer lead times.\nNotably, the ViT-LSTM outperforms ECMWF's subseasonal hindcasts beyond week 4\nby improving Scandinavian Blocking (SB) and Atlantic Ridge (AR) predictions.\nAnalysis of high-confidence predictions reveals that NAO-, SB, and AR\nopportunity forecasts can be associated with SPV variability and MJO phase\npatterns aligning with established pathways, also indicating new patterns.\nOverall, our work demonstrates that encoding physically meaningful climate\nfields can enhance S2S prediction skill, advancing AI-driven subseasonal\nforecast. Moreover, the experiments highlight the potential of deep learning\nmethods as investigative tools, providing new insights into atmospheric\ndynamics and predictability.","authors":["Philine L. Bommer","Marlene Kretschmer","Fiona R. Spuler","Kirill Bykov","Marina M. -C. H\u00f6hne"],"url":"http://arxiv.org/pdf/2504.07625v1","published":"2025-04-10"}
{"title":"Fine Flood Forecasts: Incorporating local data into global models through fine-tuning","abstract":"Floods are the most common form of natural disaster and accurate flood\nforecasting is essential for early warning systems. Previous work has shown\nthat machine learning (ML) models are a promising way to improve flood\npredictions when trained on large, geographically-diverse datasets. This\nrequirement of global training can result in a loss of ownership for national\nforecasters who cannot easily adapt the models to improve performance in their\nregion, preventing ML models from being operationally deployed. Furthermore,\ntraditional hydrology research with physics-based models suggests that local\ndata -- which in many cases is only accessible to local agencies -- is valuable\nfor improving model performance. To address these concerns, we demonstrate a\nmethodology of pre-training a model on a large, global dataset and then\nfine-tuning that model on data from individual basins. This results in\nperformance increases, validating our hypothesis that there is extra\ninformation to be captured in local data. In particular, we show that\nperformance increases are most significant in watersheds that underperform\nduring global training. We provide a roadmap for national forecasters who wish\nto take ownership of global models using their own data, aiming to lower the\nbarrier to operational deployment of ML-based hydrological forecast systems.","authors":["Emil Ryd","Grey Nearing"],"url":"http://arxiv.org/pdf/2504.12559v1","published":"2025-04-17"}
{"title":"Scalable Permutation-Aware Modeling for Temporal Set Prediction","abstract":"Temporal set prediction involves forecasting the elements that will appear in\nthe next set, given a sequence of prior sets, each containing a variable number\nof elements. Existing methods often rely on intricate architectures with\nsubstantial computational overhead, which hampers their scalability. In this\nwork, we introduce a novel and scalable framework that leverages\npermutation-equivariant and permutation-invariant transformations to\nefficiently model set dynamics. Our approach significantly reduces both\ntraining and inference time while maintaining competitive performance.\nExtensive experiments on multiple public benchmarks show that our method\nachieves results on par with or superior to state-of-the-art models across\nseveral evaluation metrics. These results underscore the effectiveness of our\nmodel in enabling efficient and scalable temporal set prediction.","authors":["Ashish Ranjan","Ayush Agarwal","Shalin Barot","Sushant Kumar"],"url":"http://arxiv.org/pdf/2504.17140v1","published":"2025-04-23"}
{"title":"Unlocking the Potential of Linear Networks for Irregular Multivariate Time Series Forecasting","abstract":"Time series forecasting holds significant importance across various\nindustries, including finance, transportation, energy, healthcare, and climate.\nDespite the widespread use of linear networks due to their low computational\ncost and effectiveness in modeling temporal dependencies, most existing\nresearch has concentrated on regularly sampled and fully observed multivariate\ntime series. However, in practice, we frequently encounter irregular\nmultivariate time series characterized by variable sampling intervals and\nmissing values. The inherent intra-series inconsistency and inter-series\nasynchrony in such data hinder effective modeling and forecasting with\ntraditional linear networks relying on static weights. To tackle these\nchallenges, this paper introduces a novel model named AiT. AiT utilizes an\nadaptive linear network capable of dynamically adjusting weights according to\nobservation time points to address intra-series inconsistency, thereby\nenhancing the accuracy of temporal dependencies modeling. Furthermore, by\nincorporating the Transformer module on variable semantics embeddings, AiT\nefficiently captures variable correlations, avoiding the challenge of\ninter-series asynchrony. Comprehensive experiments across four benchmark\ndatasets demonstrate the superiority of AiT, improving prediction accuracy by\n11% and decreasing runtime by 52% compared to existing state-of-the-art\nmethods.","authors":["Chengsen Wang","Qi Qi","Jingyu Wang","Haifeng Sun","Zirui Zhuang","Jianxin Liao"],"url":"http://arxiv.org/pdf/2505.00590v1","published":"2025-05-01"}
{"title":"Long-Term Individual Causal Effect Estimation via Identifiable Latent Representation Learning","abstract":"Estimating long-term causal effects by combining long-term observational and\nshort-term experimental data is a crucial but challenging problem in many\nreal-world scenarios. In existing methods, several ideal assumptions, e.g.\nlatent unconfoundedness assumption or additive equi-confounding bias\nassumption, are proposed to address the latent confounder problem raised by the\nobservational data. However, in real-world applications, these assumptions are\ntypically violated which limits their practical effectiveness. In this paper,\nwe tackle the problem of estimating the long-term individual causal effects\nwithout the aforementioned assumptions. Specifically, we propose to utilize the\nnatural heterogeneity of data, such as data from multiple sources, to identify\nlatent confounders, thereby significantly avoiding reliance on idealized\nassumptions. Practically, we devise a latent representation learning-based\nestimator of long-term causal effects. Theoretically, we establish the\nidentifiability of latent confounders, with which we further achieve long-term\neffect identification. Extensive experimental studies, conducted on multiple\nsynthetic and semi-synthetic datasets, demonstrate the effectiveness of our\nproposed method.","authors":["Ruichu Cai","Junjie Wan","Weilin Chen","Zeqin Yang","Zijian Li","Peng Zhen","Jiecheng Guo"],"url":"http://arxiv.org/pdf/2505.05192v1","published":"2025-05-08"}
{"title":"ChronoSteer: Bridging Large Language Model and Time Series Foundation Model via Synthetic Data","abstract":"Conventional forecasting methods rely on unimodal time series data, limiting\ntheir ability to exploit rich textual information. Recently, large language\nmodels (LLMs) and time series foundation models (TSFMs) have demonstrated\npowerful capability in textual reasoning and temporal modeling, respectively.\nIntegrating the strengths of both to construct a multimodal model that\nconcurrently leverages both temporal and textual information for future\ninference has emerged as a critical research challenge. To address the scarcity\nof event-series paired data, we propose a decoupled framework: an LLM is\nemployed to transform textual events into revision instructions, which are then\nused to steer the output of TSFM. To implement this framework, we introduce\nChronoSteer, a multimodal TSFM that can be steered through textual revision\ninstructions, effectively bridging LLM and TSFM. Moreover, to mitigate the\nshortage of cross-modal instruction-series paired data, we devise a two-stage\ntraining strategy based on synthetic data. In addition, we also construct a\nhigh-quality multimodal time series forecasting benchmark to address the\ninformation leakage concerns during evaluation. After integrating with an LLM,\nChronoSteer, which is trained exclusively on synthetic data, achieves a 25.7%\nimprovement in prediction accuracy compared to the unimodal backbone and a\n22.5% gain over the previous state-of-the-art multimodal method.","authors":["Chengsen Wang","Qi Qi","Zhongwen Rao","Lujia Pan","Jingyu Wang","Jianxin Liao"],"url":"http://arxiv.org/pdf/2505.10083v1","published":"2025-05-15"}
{"title":"CausalDynamics: A large-scale benchmark for structural discovery of dynamical causal models","abstract":"Causal discovery for dynamical systems poses a major challenge in fields\nwhere active interventions are infeasible. Most methods used to investigate\nthese systems and their associated benchmarks are tailored to deterministic,\nlow-dimensional and weakly nonlinear time-series data. To address these\nlimitations, we present CausalDynamics, a large-scale benchmark and extensible\ndata generation framework to advance the structural discovery of dynamical\ncausal models. Our benchmark consists of true causal graphs derived from\nthousands of coupled ordinary and stochastic differential equations as well as\ntwo idealized climate models. We perform a comprehensive evaluation of\nstate-of-the-art causal discovery algorithms for graph reconstruction on\nsystems with noisy, confounded, and lagged dynamics. CausalDynamics consists of\na plug-and-play, build-your-own coupling workflow that enables the construction\nof a hierarchy of physical systems. We anticipate that our framework will\nfacilitate the development of robust causal discovery algorithms that are\nbroadly applicable across domains while addressing their unique challenges. We\nprovide a user-friendly implementation and documentation on\nhttps://kausable.github.io/CausalDynamics.","authors":["Benjamin Herdeanu","Juan Nathaniel","Carla Roesch","Jatan Buch","Gregor Ramien","Johannes Haux","Pierre Gentine"],"url":"http://arxiv.org/pdf/2505.16620v1","published":"2025-05-22"}
{"title":"Improving Time Series Forecasting via Instance-aware Post-hoc Revision","abstract":"Time series forecasting plays a vital role in various real-world applications\nand has attracted significant attention in recent decades. While recent methods\nhave achieved remarkable accuracy by incorporating advanced inductive biases\nand training strategies, we observe that instance-level variations remain a\nsignificant challenge. These variations--stemming from distribution shifts,\nmissing data, and long-tail patterns--often lead to suboptimal forecasts for\nspecific instances, even when overall performance appears strong. To address\nthis issue, we propose a model-agnostic framework, PIR, designed to enhance\nforecasting performance through Post-forecasting Identification and Revision.\nSpecifically, PIR first identifies biased forecasting instances by estimating\ntheir accuracy. Based on this, the framework revises the forecasts using\ncontextual information, including covariates and historical time series, from\nboth local and global perspectives in a post-processing fashion. Extensive\nexperiments on real-world datasets with mainstream forecasting models\ndemonstrate that PIR effectively mitigates instance-level errors and\nsignificantly improves forecasting reliability.","authors":["Zhiding Liu","Mingyue Cheng","Guanhao Zhao","Jiqian Yang","Qi Liu","Enhong Chen"],"url":"http://arxiv.org/pdf/2505.23583v1","published":"2025-05-29"}
{"title":"Nonlinear Causal Discovery for Grouped Data","abstract":"Inferring cause-effect relationships from observational data has gained\nsignificant attention in recent years, but most methods are limited to scalar\nrandom variables. In many important domains, including neuroscience,\npsychology, social science, and industrial manufacturing, the causal units of\ninterest are groups of variables rather than individual scalar measurements.\nMotivated by these applications, we extend nonlinear additive noise models to\nhandle random vectors, establishing a two-step approach for causal graph\nlearning: First, infer the causal order among random vectors. Second, perform\nmodel selection to identify the best graph consistent with this order. We\nintroduce effective and novel solutions for both steps in the vector case,\ndemonstrating strong performance in simulations. Finally, we apply our method\nto real-world assembly line data with partial knowledge of causal ordering\namong variable groups.","authors":["Konstantin G\u00f6bler","Tobias Windisch","Mathias Drton"],"url":"http://arxiv.org/pdf/2506.05120v1","published":"2025-06-05"}
{"title":"Time-IMM: A Dataset and Benchmark for Irregular Multimodal Multivariate Time Series","abstract":"Time series data in real-world applications such as healthcare, climate\nmodeling, and finance are often irregular, multimodal, and messy, with varying\nsampling rates, asynchronous modalities, and pervasive missingness. However,\nexisting benchmarks typically assume clean, regularly sampled, unimodal data,\ncreating a significant gap between research and real-world deployment. We\nintroduce Time-IMM, a dataset specifically designed to capture cause-driven\nirregularity in multimodal multivariate time series. Time-IMM represents nine\ndistinct types of time series irregularity, categorized into trigger-based,\nconstraint-based, and artifact-based mechanisms. Complementing the dataset, we\nintroduce IMM-TSF, a benchmark library for forecasting on irregular multimodal\ntime series, enabling asynchronous integration and realistic evaluation.\nIMM-TSF includes specialized fusion modules, including a timestamp-to-text\nfusion module and a multimodality fusion module, which support both\nrecency-aware averaging and attention-based integration strategies. Empirical\nresults demonstrate that explicitly modeling multimodality on irregular time\nseries data leads to substantial gains in forecasting performance. Time-IMM and\nIMM-TSF provide a foundation for advancing time series analysis under\nreal-world conditions. The dataset is publicly available at\nhttps://www.kaggle.com/datasets/blacksnail789521/time-imm/data, and the\nbenchmark library can be accessed at\nhttps://anonymous.4open.science/r/IMMTSF_NeurIPS2025.","authors":["Ching Chang","Jeehyun Hwang","Yidan Shi","Haixin Wang","Wen-Chih Peng","Tien-Fu Chen","Wei Wang"],"url":"http://arxiv.org/pdf/2506.10412v1","published":"2025-06-12"}
{"title":"Warping and Matching Subsequences Between Time Series","abstract":"Comparing time series is essential in various tasks such as clustering and\nclassification. While elastic distance measures that allow warping provide a\nrobust quantitative comparison, a qualitative comparison on top of them is\nmissing. Traditional visualizations focus on point-to-point alignment and do\nnot convey the broader structural relationships at the level of subsequences.\nThis limitation makes it difficult to understand how and where one time series\nshifts, speeds up or slows down with respect to another. To address this, we\npropose a novel technique that simplifies the warping path to highlight,\nquantify and visualize key transformations (shift, compression, difference in\namplitude). By offering a clearer representation of how subsequences match\nbetween time series, our method enhances interpretability in time series\ncomparison.","authors":["Simiao Lin","Wannes Meert","Pieter Robberechts","Hendrik Blockeel"],"url":"http://arxiv.org/pdf/2506.15452v1","published":"2025-06-18"}
{"title":"Graph-Structured Feedback Multimodel Ensemble Online Conformal Prediction","abstract":"Online conformal prediction has demonstrated its capability to construct a\nprediction set for each incoming data point that covers the true label with a\npredetermined probability. To cope with potential distribution shift,\nmulti-model online conformal prediction has been introduced to select and\nleverage different models from a preselected candidate set. Along with the\nimproved flexibility, the choice of the preselected set also brings challenges.\nA candidate set that includes a large number of models may increase the\ncomputational complexity. In addition, the inclusion of irrelevant models with\npoor performance may negatively impact the performance and lead to\nunnecessarily large prediction sets. To address these challenges, we propose a\nnovel multi-model online conformal prediction algorithm that identifies a\nsubset of effective models at each time step by collecting feedback from a\nbipartite graph, which is refined upon receiving new data. A model is then\nselected from this subset to construct the prediction set, resulting in reduced\ncomputational complexity and smaller prediction sets. Additionally, we\ndemonstrate that using prediction set size as feedback, alongside model loss,\ncan significantly improve efficiency by constructing smaller prediction sets\nwhile still satisfying the required coverage guarantee. The proposed algorithms\nare proven to ensure valid coverage and achieve sublinear regret. Experiments\non real and synthetic datasets validate that the proposed methods construct\nsmaller prediction sets and outperform existing multi-model online conformal\nprediction approaches.","authors":["Erfan Hajihashemi","Yanning Shen"],"url":"http://arxiv.org/pdf/2506.20898v1","published":"2025-06-26"}
{"title":"Statistical Inference for Responsiveness Verification","abstract":"Many safety failures in machine learning arise when models are used to assign\npredictions to people (often in settings like lending, hiring, or content\nmoderation) without accounting for how individuals can change their inputs. In\nthis work, we introduce a formal validation procedure for the responsiveness of\npredictions with respect to interventions on their features. Our procedure\nframes responsiveness as a type of sensitivity analysis in which practitioners\ncontrol a set of changes by specifying constraints over interventions and\ndistributions over downstream effects. We describe how to estimate\nresponsiveness for the predictions of any model and any dataset using only\nblack-box access, and how to use these estimates to support tasks such as\nfalsification and failure probability estimation. We develop algorithms that\nconstruct these estimates by generating a uniform sample of reachable points,\nand demonstrate how they can promote safety in real-world applications such as\nrecidivism prediction, organ transplant prioritization, and content moderation.","authors":["Seung Hyun Cheon","Meredith Stewart","Bogdan Kulynych","Tsui-Wei Weng","Berk Ustun"],"url":"http://arxiv.org/pdf/2507.02169v1","published":"2025-07-02"}
{"title":"Efficient Causal Discovery for Autoregressive Time Series","abstract":"In this study, we present a novel constraint-based algorithm for causal\nstructure learning specifically designed for nonlinear autoregressive time\nseries. Our algorithm significantly reduces computational complexity compared\nto existing methods, making it more efficient and scalable to larger problems.\nWe rigorously evaluate its performance on synthetic datasets, demonstrating\nthat our algorithm not only outperforms current techniques, but also excels in\nscenarios with limited data availability. These results highlight its potential\nfor practical applications in fields requiring efficient and accurate causal\ninference from nonlinear time series data.","authors":["Mohammad Fesanghary","Achintya Gopal"],"url":"http://arxiv.org/pdf/2507.07898v1","published":"2025-07-10"}
{"title":"MoTM: Towards a Foundation Model for Time Series Imputation based on Continuous Modeling","abstract":"Recent years have witnessed a growing interest for time series foundation\nmodels, with a strong emphasis on the forecasting task. Yet, the crucial task\nof out-of-domain imputation of missing values remains largely underexplored. We\npropose a first step to fill this gap by leveraging implicit neural\nrepresentations (INRs). INRs model time series as continuous functions and\nnaturally handle various missing data scenarios and sampling rates. While they\nhave shown strong performance within specific distributions, they struggle\nunder distribution shifts. To address this, we introduce MoTM (Mixture of\nTimeflow Models), a step toward a foundation model for time series imputation.\nBuilding on the idea that a new time series is a mixture of previously seen\npatterns, MoTM combines a basis of INRs, each trained independently on a\ndistinct family of time series, with a ridge regressor that adapts to the\nobserved context at inference. We demonstrate robust in-domain and\nout-of-domain generalization across diverse imputation scenarios (e.g., block\nand pointwise missingness, variable sampling rates), paving the way for\nadaptable foundation imputation models.","authors":["Etienne Le Naour","Tahar Nabil","Ghislain Agoua"],"url":"http://arxiv.org/pdf/2507.13207v1","published":"2025-07-17"}
{"title":"ChronoSelect: Robust Learning with Noisy Labels via Dynamics Temporal Memory","abstract":"Training deep neural networks on real-world datasets is often hampered by the\npresence of noisy labels, which can be memorized by over-parameterized models,\nleading to significant degradation in generalization performance. While\nexisting methods for learning with noisy labels (LNL) have made considerable\nprogress, they fundamentally suffer from static snapshot evaluations and fail\nto leverage the rich temporal dynamics of learning evolution. In this paper, we\npropose ChronoSelect (chrono denoting its temporal nature), a novel framework\nfeaturing an innovative four-stage memory architecture that compresses\nprediction history into compact temporal distributions. Our unique sliding\nupdate mechanism with controlled decay maintains only four dynamic memory units\nper sample, progressively emphasizing recent patterns while retaining essential\nhistorical knowledge. This enables precise three-way sample partitioning into\nclean, boundary, and noisy subsets through temporal trajectory analysis and\ndual-branch consistency. Theoretical guarantees prove the mechanism's\nconvergence and stability under noisy conditions. Extensive experiments\ndemonstrate ChronoSelect's state-of-the-art performance across synthetic and\nreal-world benchmarks.","authors":["Jianchao Wang","Qingfeng Li","Pengcheng Zheng","Xiaorong Pu","Yazhou Ren"],"url":"http://arxiv.org/pdf/2507.18183v1","published":"2025-07-24"}
{"title":"Observational Multiplicity","abstract":"Many prediction tasks can admit multiple models that can perform almost\nequally well. This phenomenon can can undermine interpretability and safety\nwhen competing models assign conflicting predictions to individuals. In this\nwork, we study how arbitrariness can arise in probabilistic classification\ntasks as a result of an effect that we call \\emph{observational multiplicity}.\nWe discuss how this effect arises in a broad class of practical applications\nwhere we learn a classifier to predict probabilities $p_i \\in [0,1]$ but are\ngiven a dataset of observations $y_i \\in \\{0,1\\}$. We propose to evaluate the\narbitrariness of individual probability predictions through the lens of\n\\emph{regret}. We introduce a measure of regret for probabilistic\nclassification tasks, which measures how the predictions of a model could\nchange as a result of different training labels change. We present a\ngeneral-purpose method to estimate the regret in a probabilistic classification\ntask. We use our measure to show that regret is higher for certain groups in\nthe dataset and discuss potential applications of regret. We demonstrate how\nestimating regret promote safety in real-world applications by abstention and\ndata collection.","authors":["Erin George","Deanna Needell","Berk Ustun"],"url":"http://arxiv.org/pdf/2507.23136v1","published":"2025-07-30"}
{"title":"Q-DPTS: Quantum Differentially Private Time Series Forecasting via Variational Quantum Circuits","abstract":"Time series forecasting is vital in domains where data sensitivity is\nparamount, such as finance and energy systems. While Differential Privacy (DP)\nprovides theoretical guarantees to protect individual data contributions, its\nintegration especially via DP-SGD often impairs model performance due to\ninjected noise. In this paper, we propose Q-DPTS, a hybrid quantum-classical\nframework for Quantum Differentially Private Time Series Forecasting. Q-DPTS\ncombines Variational Quantum Circuits (VQCs) with per-sample gradient clipping\nand Gaussian noise injection, ensuring rigorous $(\\epsilon,\n\\delta)$-differential privacy. The expressiveness of quantum models enables\nimproved robustness against the utility loss induced by DP mechanisms. We\nevaluate Q-DPTS on the ETT (Electricity Transformer Temperature) dataset, a\nstandard benchmark for long-term time series forecasting. Our approach is\ncompared against both classical and quantum baselines, including LSTM, QASA,\nQRWKV, and QLSTM. Results demonstrate that Q-DPTS consistently achieves lower\nprediction error under the same privacy budget, indicating a favorable\nprivacy-utility trade-off. This work presents one of the first explorations\ninto quantum-enhanced differentially private forecasting, offering promising\ndirections for secure and accurate time series modeling in privacy-critical\nscenarios.","authors":["Chi-Sheng Chen","Samuel Yen-Chi Chen"],"url":"http://arxiv.org/pdf/2508.05036v1","published":"2025-08-07"}
{"title":"Self-Supervised Temporal Super-Resolution of Energy Data using Generative Adversarial Transformer","abstract":"To bridge the temporal granularity gap in energy network design and operation\nbased on Energy System Models, resampling of time series is required. While\nconventional upsampling methods are computationally efficient, they often\nresult in significant information loss or increased noise. Advanced models such\nas time series generation models, Super-Resolution models and imputation models\nshow potential, but also face fundamental challenges. The goal of time series\ngenerative models is to learn the distribution of the original data to generate\nhigh-resolution series with similar statistical characteristics. This is not\nentirely consistent with the definition of upsampling. Time series\nSuper-Resolution models or imputation models can degrade the accuracy of\nupsampling because the input low-resolution time series are sparse and may have\ninsufficient context. Moreover, such models usually rely on supervised learning\nparadigms. This presents a fundamental application paradox: their training\nrequires the high-resolution time series that is intrinsically absent in\nupsampling application scenarios. To address the mentioned upsampling issue,\nthis paper introduces a new method utilizing Generative Adversarial\nTransformers (GATs), which can be trained without access to any ground-truth\nhigh-resolution data. Compared with conventional interpolation methods, the\nintroduced method can reduce the root mean square error (RMSE) of upsampling\ntasks by 9%, and the accuracy of a model predictive control (MPC) application\nscenario is improved by 13%.","authors":["Xuanhao Mu","G\u00f6khan Demirel","Yuzhe Zhang","Jianlei Liu","Thorsten Schlachter","Veit Hagenmeyer"],"url":"http://arxiv.org/pdf/2508.10587v1","published":"2025-08-14"}
{"title":"Enhancing Forecasting with a 2D Time Series Approach for Cohort-Based Data","abstract":"This paper introduces a novel two-dimensional (2D) time series forecasting\nmodel that integrates cohort behavior over time, addressing challenges in small\ndata environments. We demonstrate its efficacy using multiple real-world\ndatasets, showcasing superior performance in accuracy and adaptability compared\nto reference models. The approach offers valuable insights for strategic\ndecision-making across industries facing financial and marketing forecasting\nchallenges.","authors":["Yonathan Guttel","Orit Moradov","Nachi Lieder","Asnat Greenstein-Messica"],"url":"http://arxiv.org/pdf/2508.15369v1","published":"2025-08-21"}
{"title":"Online time series prediction using feature adjustment","abstract":"Time series forecasting is of significant importance across various domains.\nHowever, it faces significant challenges due to distribution shift. This issue\nbecomes particularly pronounced in online deployment scenarios where data\narrives sequentially, requiring models to adapt continually to evolving\npatterns. Current time series online learning methods focus on two main\naspects: selecting suitable parameters to update (e.g., final layer weights or\nadapter modules) and devising suitable update strategies (e.g., using recent\nbatches, replay buffers, or averaged gradients). We challenge the conventional\nparameter selection approach, proposing that distribution shifts stem from\nchanges in underlying latent factors influencing the data. Consequently,\nupdating the feature representations of these latent factors may be more\neffective. To address the critical problem of delayed feedback in multi-step\nforecasting (where true values arrive much later than predictions), we\nintroduce ADAPT-Z (Automatic Delta Adjustment via Persistent Tracking in\nZ-space). ADAPT-Z utilizes an adapter module that leverages current feature\nrepresentations combined with historical gradient information to enable robust\nparameter updates despite the delay. Extensive experiments demonstrate that our\nmethod consistently outperforms standard base models without adaptation and\nsurpasses state-of-the-art online learning approaches across multiple datasets.\nThe code is available at https://github.com/xiannanhuang/ADAPT-Z.","authors":["Xiannan Huang","Shuhan Qiu","Jiayuan Du","Chao Yang"],"url":"http://arxiv.org/pdf/2509.03810v1","published":"2025-09-04"}
{"title":"CountTRuCoLa: Rule Confidence Learning for Temporal Knowledge Graph Forecasting","abstract":"We address the task of temporal knowledge graph (TKG) forecasting by\nintroducing a fully explainable method based on temporal rules. Motivated by\nrecent work proposing a strong baseline using recurrent facts, our approach\nlearns four simple types of rules with a confidence function that considers\nboth recency and frequency. Evaluated on nine datasets, our method matches or\nsurpasses the performance of eight state-of-the-art models and two baselines,\nwhile providing fully interpretable predictions.","authors":["Julia Gastinger","Christian Meilicke","Heiner Stuckenschmidt"],"url":"http://arxiv.org/pdf/2509.09474v1","published":"2025-09-11"}
{"title":"DAG: A Dual Causal Network for Time Series Forecasting with Exogenous Variables","abstract":"Time series forecasting is crucial in various fields such as economics,\ntraffic, and AIOps. However, in real-world applications, focusing solely on the\nendogenous variables (i.e., target variables), is often insufficient to ensure\naccurate predictions. Considering exogenous variables (i.e., covariates)\nprovides additional predictive information, thereby improving forecasting\naccuracy. However, existing methods for time series forecasting with exogenous\nvariables (TSF-X) have the following shortcomings: 1) they do not leverage\nfuture exogenous variables, 2) they fail to account for the causal\nrelationships between endogenous and exogenous variables. As a result, their\nperformance is suboptimal. In this study, to better leverage exogenous\nvariables, especially future exogenous variable, we propose a general framework\nDAG, which utilizes dual causal network along both the temporal and channel\ndimensions for time series forecasting with exogenous variables. Specifically,\nwe first introduce the Temporal Causal Module, which includes a causal\ndiscovery module to capture how historical exogenous variables affect future\nexogenous variables. Following this, we construct a causal injection module\nthat incorporates the discovered causal relationships into the process of\nforecasting future endogenous variables based on historical endogenous\nvariables. Next, we propose the Channel Causal Module, which follows a similar\ndesign principle. It features a causal discovery module models how historical\nexogenous variables influence historical endogenous variables, and a causal\ninjection module incorporates the discovered relationships to enhance the\nprediction of future endogenous variables based on future exogenous variables.","authors":["Xiangfei Qiu","Yuhan Zhu","Zhengyu Li","Hanyin Cheng","Xingjian Wu","Chenjuan Guo","Bin Yang","Jilin Hu"],"url":"http://arxiv.org/pdf/2509.14933v1","published":"2025-09-18"}
{"title":"Lossless Compression: A New Benchmark for Time Series Model Evaluation","abstract":"The evaluation of time series models has traditionally focused on four\ncanonical tasks: forecasting, imputation, anomaly detection, and\nclassification. While these tasks have driven significant progress, they\nprimarily assess task-specific performance and do not rigorously measure\nwhether a model captures the full generative distribution of the data. We\nintroduce lossless compression as a new paradigm for evaluating time series\nmodels, grounded in Shannon's source coding theorem. This perspective\nestablishes a direct equivalence between optimal compression length and the\nnegative log-likelihood, providing a strict and unified information-theoretic\ncriterion for modeling capacity. Then We define a standardized evaluation\nprotocol and metrics. We further propose and open-source a comprehensive\nevaluation framework TSCom-Bench, which enables the rapid adaptation of time\nseries models as backbones for lossless compression. Experiments across diverse\ndatasets on state-of-the-art models, including TimeXer, iTransformer, and\nPatchTST, demonstrate that compression reveals distributional weaknesses\noverlooked by classic benchmarks. These findings position lossless compression\nas a principled task that complements and extends existing evaluation for time\nseries modeling.","authors":["Meng Wan","Benxi Tian","Jue Wang","Cui Hui","Ningming Nie","Tiantian Liu","Zongguo Wang","Cao Rongqiang","Peng Shi","Yangang Wang"],"url":"http://arxiv.org/pdf/2509.21002v1","published":"2025-09-25"}
{"title":"KAIROS: Unified Training for Universal Non-Autoregressive Time Series Forecasting","abstract":"In the World Wide Web, reliable time series forecasts provide the\nforward-looking signals that drive resource planning, cache placement, and\nanomaly response, enabling platforms to operate efficiently as user behavior\nand content distributions evolve. Compared with other domains, time series\nforecasting for Web applications requires much faster responsiveness to support\nreal-time decision making. We present KAIROS, a non-autoregressive time series\nforecasting framework that directly models segment-level multi-peak\ndistributions. Unlike autoregressive approaches, KAIROS avoids error\naccumulation and achieves just-in-time inference, while improving over existing\nnon-autoregressive models that collapse to over-smoothed predictions. Trained\non the large-scale corpus, KAIROS demonstrates strong zero-shot generalization\non six widely used benchmarks, delivering forecasting performance comparable to\nstate-of-the-art foundation models with similar scale, at a fraction of their\ninference cost. Beyond empirical results, KAIROS highlights the importance of\nnon-autoregressive design as a scalable paradigm for foundation models in time\nseries.","authors":["Kuiye Ding","Fanda Fan","Zheya Wang","Hongxiao Li","Yifan Wang","Lei Wang","Chunjie Luo","Jianfeng Zhan"],"url":"http://arxiv.org/pdf/2510.02084v1","published":"2025-10-02"}
{"title":"Tackling Time-Series Forecasting Generalization via Mitigating Concept Drift","abstract":"Time-series forecasting finds broad applications in real-world scenarios. Due\nto the dynamic nature of time series data, it is important for time-series\nforecasting models to handle potential distribution shifts over time. In this\npaper, we initially identify two types of distribution shifts in time series:\nconcept drift and temporal shift. We acknowledge that while existing studies\nprimarily focus on addressing temporal shift issues in time series forecasting,\ndesigning proper concept drift methods for time series forecasting has received\ncomparatively less attention.\n  Motivated by the need to address potential concept drift, while conventional\nconcept drift methods via invariant learning face certain challenges in\ntime-series forecasting, we propose a soft attention mechanism that finds\ninvariant patterns from both lookback and horizon time series. Additionally, we\nemphasize the critical importance of mitigating temporal shifts as a\npreliminary to addressing concept drift. In this context, we introduce ShifTS,\na method-agnostic framework designed to tackle temporal shift first and then\nconcept drift within a unified approach. Extensive experiments demonstrate the\nefficacy of ShifTS in consistently enhancing the forecasting accuracy of\nagnostic models across multiple datasets, and outperforming existing concept\ndrift, temporal shift, and combined baselines.","authors":["Zhiyuan Zhao","Haoxin Liu","B. Aditya Prakash"],"url":"http://arxiv.org/pdf/2510.14814v1","published":"2025-10-16"}
{"title":"xTime: Extreme Event Prediction with Hierarchical Knowledge Distillation and Expert Fusion","abstract":"Extreme events frequently occur in real-world time series and often carry\nsignificant practical implications. In domains such as climate and healthcare,\nthese events, such as floods, heatwaves, or acute medical episodes, can lead to\nserious consequences. Accurate forecasting of such events is therefore of\nsubstantial importance. Most existing time series forecasting models are\noptimized for overall performance within the prediction window, but often\nstruggle to accurately predict extreme events, such as high temperatures or\nheart rate spikes. The main challenges are data imbalance and the neglect of\nvaluable information contained in intermediate events that precede extreme\nevents. In this paper, we propose xTime, a novel framework for extreme event\nforecasting in time series. xTime leverages knowledge distillation to transfer\ninformation from models trained on lower-rarity events, thereby improving\nprediction performance on rarer ones. In addition, we introduce a mixture of\nexperts (MoE) mechanism that dynamically selects and fuses outputs from expert\nmodels across different rarity levels, which further improves the forecasting\nperformance for extreme events. Experiments on multiple datasets show that\nxTime achieves consistent improvements, with forecasting accuracy on extreme\nevents improving from 3% to 78%.","authors":["Quan Li","Wenchao Yu","Suhang Wang","Minhua Lin","Lingwei Chen","Wei Cheng","Haifeng Chen"],"url":"http://arxiv.org/pdf/2510.20651v1","published":"2025-10-23"}
{"title":"Towards Explainable and Reliable AI in Finance","abstract":"Financial forecasting increasingly uses large neural network models, but\ntheir opacity raises challenges for trust and regulatory compliance. We present\nseveral approaches to explainable and reliable AI in finance. \\emph{First}, we\ndescribe how Time-LLM, a time series foundation model, uses a prompt to avoid a\nwrong directional forecast. \\emph{Second}, we show that combining foundation\nmodels for time series forecasting with a reliability estimator can filter our\nunreliable predictions. \\emph{Third}, we argue for symbolic reasoning encoding\ndomain rules for transparent justification. These approaches shift emphasize\nexecuting only forecasts that are both reliable and explainable. Experiments on\nequity and cryptocurrency data show that the architecture reduces false\npositives and supports selective execution. By integrating predictive\nperformance with reliability estimation and rule-based reasoning, our framework\nadvances transparent and auditable financial AI systems.","authors":["Albi Isufaj","Pablo Moll\u00e1","Helmut Prendinger"],"url":"http://arxiv.org/pdf/2510.26353v1","published":"2025-10-30"}
{"title":"Towards Causal Market Simulators","abstract":"Market generators using deep generative models have shown promise for\nsynthetic financial data generation, but existing approaches lack causal\nreasoning capabilities essential for counterfactual analysis and risk\nassessment. We propose a Time-series Neural Causal Model VAE (TNCM-VAE) that\ncombines variational autoencoders with structural causal models to generate\ncounterfactual financial time series while preserving both temporal\ndependencies and causal relationships. Our approach enforces causal constraints\nthrough directed acyclic graphs in the decoder architecture and employs the\ncausal Wasserstein distance for training. We validate our method on synthetic\nautoregressive models inspired by the Ornstein-Uhlenbeck process, demonstrating\nsuperior performance in counterfactual probability estimation with L1 distances\nas low as 0.03-0.10 compared to ground truth. The model enables financial\nstress testing, scenario analysis, and enhanced backtesting by generating\nplausible counterfactual market trajectories that respect underlying causal\nmechanisms.","authors":["Dennis Thumm","Luis Ontaneda Mijares"],"url":"http://arxiv.org/pdf/2511.04469v1","published":"2025-11-06"}
{"title":"User Traffic Prediction for Proactive Resource Management: Learning-Powered Approaches","abstract":"Traffic prediction plays a vital role in efficient planning and usage of network resources in wireless networks. While traffic prediction in wired networks is an established field, there is a lack of research on the analysis of traffic in cellular networks, especially in a content-blind manner at the user level. Here, we shed light into this problem by designing traffic prediction tools that employ either statistical, rule-based, or deep machine learning methods. First, we present an extensive experimental evaluation of the designed tools over a real traffic dataset. Within this analysis, the impact of different parameters, such as length of prediction, feature set used in analyses, and granularity of data, on accuracy of prediction are investigated. Second, regarding the coupling observed between behavior of traffic and its generating application, we extend our analysis to the blind classification of applications generating the traffic based on the statistics of traffic arrival/departure. The results demonstrate presence of a threshold number of previous observations, beyond which, deep machine learning can outperform linear statistical learning, and before which, statistical learning outperforms deep learning approaches. Further analysis of this threshold value represents a strong coupling between this threshold, the length of future prediction, and the feature set in use. Finally, through a case study, we present how the experienced delay could be decreased by traffic arrival prediction.","authors":["Amin Azari","Panagiotis Papapetrou","Stojan Denic","Gunnar Peters"],"url":"https://arxiv.org/pdf/1906.00951v1","published":"2019-05-09"}
{"title":"FreqFlow: Long-term forecasting using lightweight flow matching","abstract":"Multivariate time-series (MTS) forecasting is fundamental to applications ranging from urban mobility and resource management to climate modeling. While recent generative models based on denoising diffusion have advanced state-of-the-art performance in capturing complex data distributions, they suffer from significant computational overhead due to iterative stochastic sampling procedures that limit real-time deployment. Moreover, these models can be brittle when handling high-dimensional, non-stationary, and multi-scale periodic patterns characteristic of real-world sensor networks. We introduce FreqFlow, a novel framework that leverages conditional flow matching in the frequency domain for deterministic MTS forecasting. Unlike conventional approaches that operate in the time domain, FreqFlow transforms the forecasting problem into the spectral domain, where it learns to model amplitude and phase shifts through a single complex-valued linear layer. This frequency-domain formulation enables the model to efficiently capture temporal dynamics via complex multiplication, corresponding to scaling and temporal translations. The resulting architecture is exceptionally lightweight with only 89k parameters - an order of magnitude smaller than competing diffusion-based models-while enabling single-pass deterministic sampling through ordinary differential equation (ODE) integration. Our approach decomposes MTS signals into trend, seasonal, and residual components, with the flow matching mechanism specifically designed for residual learning to enhance long-term forecasting accuracy. Extensive experiments on real-world traffic speed, volume, and flow datasets demonstrate that FreqFlow achieves state-of-the-art forecasting performance, on average 7\\% RMSE improvements, while being significantly faster and more parameter-efficient than existing methods","authors":["Seyed Mohamad Moghadas","Bruno Cornelis","Adrian Munteanu"],"url":"https://arxiv.org/pdf/2511.16426v1","published":"2025-11-20"}
{"title":"Context-Specific Causal Graph Discovery with Unobserved Contexts: Non-Stationarity, Regimes and Spatio-Temporal Patterns","abstract":"Real-world data, for example in climate applications, often consists of spatially gridded time series data or data with comparable structure. While the underlying system is often believed to behave similar at different points in space and time, those variations that do exist are twofold relevant: They often encode important information in and of themselves. And they may negatively affect the stability / convergence and reliability\\Slash{}validity of results of algorithms assuming stationarity or space-translation invariance. We study the information encoded in changes of the causal graph, with stability in mind. An analysis of this general task identifies two core challenges. We develop guiding principles to overcome these challenges, and provide a framework realizing these principles by modifying constraint-based causal discovery approaches on the level of independence testing. This leads to an extremely modular, easily extensible and widely applicable framework. It can leverage existing constraint-based causal discovery methods (demonstrated on IID-algorithms PC, PC-stable, FCI and time series algorithms PCMCI, PCMCI+, LPCMCI) with little to no modification. The built-in modularity allows to systematically understand and improve upon an entire array of subproblems. By design, it can be extended by leveraging insights from change-point-detection, clustering, independence-testing and other well-studied related problems. The division into more accessible sub-problems also simplifies the understanding of fundamental limitations, hyperparameters controlling trade-offs and the statistical interpretation of results. An open-source implementation will be available soon.","authors":["Martin Rabel","Jakob Runge"],"url":"https://arxiv.org/pdf/2511.21537v1","published":"2025-11-26"}
{"title":"STELLA: Guiding Large Language Models for Time Series Forecasting with Semantic Abstractions","abstract":"Recent adaptations of Large Language Models (LLMs) for time series forecasting often fail to effectively enhance information for raw series, leaving LLM reasoning capabilities underutilized. Existing prompting strategies rely on static correlations rather than generative interpretations of dynamic behavior, lacking critical global and instance-specific context. To address this, we propose STELLA (Semantic-Temporal Alignment with Language Abstractions), a framework that systematically mines and injects structured supplementary and complementary information. STELLA employs a dynamic semantic abstraction mechanism that decouples input series into trend, seasonality, and residual components. It then translates intrinsic behavioral features of these components into Hierarchical Semantic Anchors: a Corpus-level Semantic Prior (CSP) for global context and a Fine-grained Behavioral Prompt (FBP) for instance-level patterns. Using these anchors as prefix-prompts, STELLA guides the LLM to model intrinsic dynamics. Experiments on eight benchmark datasets demonstrate that STELLA outperforms state-of-the-art methods in long- and short-term forecasting, showing superior generalization in zero-shot and few-shot settings. Ablation studies further validate the effectiveness of our dynamically generated semantic anchors.","authors":["Junjie Fan","Hongye Zhao","Linduo Wei","Jiayu Rao","Guijia Li","Jiaxin Yuan","Wenqi Xu","Yong Qi"],"url":"https://arxiv.org/pdf/2512.04871v1","published":"2025-12-04"}
{"title":"Adaptive Information Routing for Multimodal Time Series Forecasting","abstract":"Time series forecasting is a critical task for artificial intelligence with numerous real-world applications. Traditional approaches primarily rely on historical time series data to predict the future values. However, in practical scenarios, this is often insufficient for accurate predictions due to the limited information available. To address this challenge, multimodal time series forecasting methods which incorporate additional data modalities, mainly text data, alongside time series data have been explored. In this work, we introduce the Adaptive Information Routing (AIR) framework, a novel approach for multimodal time series forecasting. Unlike existing methods that treat text data on par with time series data as interchangeable auxiliary features for forecasting, AIR leverages text information to dynamically guide the time series model by controlling how and to what extent multivariate time series information should be combined. We also present a text-refinement pipeline that employs a large language model to convert raw text data into a form suitable for multimodal forecasting, and we introduce a benchmark that facilitates multimodal forecasting experiments based on this pipeline. Experiment results with the real world market data such as crude oil price and exchange rates demonstrate that AIR effectively modulates the behavior of the time series model using textual inputs, significantly enhancing forecasting accuracy in various time series forecasting tasks.","authors":["Jun Seo","Hyeokjun Choe","Seohui Bae","Soyeon Park","Wonbin Ahn","Taeyoon Lim","Junhyuk Kang","Sangjun Han","Jaehoon Lee","Dongwan Kang","Minjae Kim","Sungdong Yoo","Soonyoung Lee"],"url":"https://arxiv.org/pdf/2512.10229v1","published":"2025-12-11"}
{"title":"CauSTream: Causal Spatio-Temporal Representation Learning for Streamflow Forecasting","abstract":"Streamflow forecasting is crucial for water resource management and risk mitigation. While deep learning models have achieved strong predictive performance, they often overlook underlying physical processes, limiting interpretability and generalization. Recent causal learning approaches address these issues by integrating domain knowledge, yet they typically rely on fixed causal graphs that fail to adapt to data. We propose CauStream, a unified framework for causal spatiotemporal streamflow forecasting. CauSTream jointly learns (i) a runoff causal graph among meteorological forcings and (ii) a routing graph capturing dynamic dependencies across stations. We further establish identifiability conditions for these causal structures under a nonparametric setting. We evaluate CauSTream on three major U.S. river basins across three forecasting horizons. The model consistently outperforms prior state-of-the-art methods, with performance gaps widening at longer forecast windows, indicating stronger generalization to unseen conditions. Beyond forecasting, CauSTream also learns causal graphs that capture relationships among hydrological factors and stations. The inferred structures align closely with established domain knowledge, offering interpretable insights into watershed dynamics. CauSTream offers a principled foundation for causal spatiotemporal modeling, with the potential to extend to a wide range of scientific and environmental applications.","authors":["Shu Wan","Reepal Shah","John Sabo","Huan Liu","K. Sel\u00e7uk Candan"],"url":"https://arxiv.org/pdf/2512.16046v1","published":"2025-12-18"}
{"title":"TS-Arena Technical Report -- A Pre-registered Live Forecasting Platform","abstract":"While Time Series Foundation Models (TSFMs) offer transformative capabilities for forecasting, they simultaneously risk triggering a fundamental evaluation crisis. This crisis is driven by information leakage due to overlapping training and test sets across different models, as well as the illegitimate transfer of global patterns to test data. While the ability to learn shared temporal dynamics represents a primary strength of these models, their evaluation on historical archives often permits the exploitation of observed global shocks, which violates the independence required for valid benchmarking. We introduce TS-Arena, a platform that restores the operational integrity of forecasting by treating the genuinely unknown future as the definitive test environment. By implementing a pre-registration mechanism on live data streams, the platform ensures that evaluation targets remain physically non-existent during inference, thereby enforcing a strict global temporal split. This methodology establishes a moving temporal frontier that prevents historical contamination and provides an authentic assessment of model generalization. Initially applied within the energy sector, TS-Arena provides a sustainable infrastructure for comparing foundation models under real-world constraints. A prototype of the platform is available at https://huggingface.co/spaces/DAG-UPB/TS-Arena.","authors":["Marcel Meyer","Sascha Kaltenpoth","Kevin Zalipski","Henrik Albers","Oliver M\u00fcller"],"url":"https://arxiv.org/pdf/2512.20761v1","published":"2025-12-23"}
{"title":"PRISM: A hierarchical multiscale approach for time series forecasting","abstract":"Forecasting is critical in areas such as finance, biology, and healthcare. Despite the progress in the field, making accurate forecasts remains challenging because real-world time series contain both global trends, local fine-grained structure, and features on multiple scales in between. Here, we present a new forecasting method, PRISM (Partitioned Representation for Iterative Sequence Modeling), that addresses this challenge through a learnable tree-based partitioning of the signal. At the root of the tree, a global representation captures coarse trends in the signal, while recursive splits reveal increasingly localized views of the signal. At each level of the tree, data are projected onto a time-frequency basis (e.g., wavelets or exponential moving averages) to extract scale-specific features, which are then aggregated across the hierarchy. This design allows the model to jointly capture global structure and local dynamics of the signal, enabling accurate forecasting. Experiments across benchmark datasets show that our method outperforms state-of-the-art methods for forecasting. Overall, these results demonstrate that our hierarchical approach provides a lightweight and flexible framework for forecasting multivariate time series. The code is available at https://github.com/nerdslab/prism.","authors":["Zihao Chen","Alexandre Andre","Wenrui Ma","Ian Knight","Sergey Shuvaev","Eva Dyer"],"url":"https://arxiv.org/pdf/2512.24898v1","published":"2025-12-31"}
{"title":"FaST: Efficient and Effective Long-Horizon Forecasting for Large-Scale Spatial-Temporal Graphs via Mixture-of-Experts","abstract":"Spatial-Temporal Graph (STG) forecasting on large-scale networks has garnered significant attention. However, existing models predominantly focus on short-horizon predictions and suffer from notorious computational costs and memory consumption when scaling to long-horizon predictions and large graphs. Targeting the above challenges, we present FaST, an effective and efficient framework based on heterogeneity-aware Mixture-of-Experts (MoEs) for long-horizon and large-scale STG forecasting, which unlocks one-week-ahead (672 steps at a 15-minute granularity) prediction with thousands of nodes. FaST is underpinned by two key innovations. First, an adaptive graph agent attention mechanism is proposed to alleviate the computational burden inherent in conventional graph convolution and self-attention modules when applied to large-scale graphs. Second, we propose a new parallel MoE module that replaces traditional feed-forward networks with Gated Linear Units (GLUs), enabling an efficient and scalable parallel structure. Extensive experiments on real-world datasets demonstrate that FaST not only delivers superior long-horizon predictive accuracy but also achieves remarkable computational efficiency compared to state-of-the-art baselines. Our source code is available at: https://github.com/yijizhao/FaST.","authors":["Yiji Zhao","Zihao Zhong","Ao Wang","Haomin Wen","Ming Jin","Yuxuan Liang","Huaiyu Wan","Hao Wu"],"url":"https://arxiv.org/pdf/2601.05174v1","published":"2026-01-08"}
{"title":"ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition","abstract":"Time Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: current approaches either rely on restrictive distributional assumptions, conflate different sources of uncertainty, or lack principled calibration mechanisms. While recent TSFMs employ sophisticated techniques such as mixture models, Student's t-distributions, or conformal prediction, they fail to address the core challenge of providing theoretically-grounded uncertainty decomposition. For the very first time, we present a novel transformer-based probabilistic framework, ProbFM (probabilistic foundation model), that leverages Deep Evidential Regression (DER) to provide principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches that pre-specify distributional forms or require sampling-based inference, ProbFM learns optimal uncertainty representations through higher-order evidence learning while maintaining single-pass computational efficiency. To rigorously evaluate the core DER uncertainty quantification approach independent of architectural complexity, we conduct an extensive controlled comparison study using a consistent LSTM architecture across five probabilistic methods: DER, Gaussian NLL, Student's-t NLL, Quantile Loss, and Conformal Prediction. Evaluation on cryptocurrency return forecasting demonstrates that DER maintains competitive forecasting accuracy while providing explicit epistemic-aleatoric uncertainty decomposition. This work establishes both an extensible framework for principled uncertainty quantification in foundation models and empirical evidence for DER's effectiveness in financial applications.","authors":["Arundeep Chinta","Lucas Vinh Tran","Jay Katukuri"],"url":"https://arxiv.org/pdf/2601.10591v1","published":"2026-01-15"}
{"title":"Assessing the informative value of macroeconomic indicators for public health forecasting","abstract":"Macroeconomic conditions influence the environments in which health systems operate, yet their value as leading signals of health system capacity has not been systematically evaluated. In this study, we examine whether selected macroeconomic indicators contain predictive information for several capacity-related public health targets, including employment in the health and social assistance workforce, new business applications in the sector, and health care construction spending. Using monthly U.S. time series data, we evaluate multiple forecasting approaches, including neural network models with different optimization strategies, generalized additive models, random forests, and time series models with exogenous macroeconomic indicators, under alternative model fitting designs. Across evaluation settings, we find that macroeconomic indicators provide a consistent and reproducible predictive signal for some public health targets, particularly workforce and infrastructure measures, while other targets exhibit weaker or less stable predictability. Models emphasizing stability and implicit regularization tend to perform more reliably during periods of economic volatility. These findings suggest that macroeconomic indicators may serve as useful upstream signals for digital public health monitoring, while underscoring the need for careful model selection and validation when translating economic trends into health system forecasting tools.","authors":["Shome Chakraborty","Fardil Khan","Soutik Ghosal"],"url":"https://arxiv.org/pdf/2601.15514v1","published":"2026-01-21"}
{"title":"MoHETS: Long-term Time Series Forecasting with Mixture-of-Heterogeneous-Experts","abstract":"Real-world multivariate time series can exhibit intricate multi-scale structures, including global trends, local periodicities, and non-stationary regimes, which makes long-horizon forecasting challenging. Although sparse Mixture-of-Experts (MoE) approaches improve scalability and specialization, they typically rely on homogeneous MLP experts that poorly capture the diverse temporal dynamics of time series data. We address these limitations with MoHETS, an encoder-only Transformer that integrates sparse Mixture-of-Heterogeneous-Experts (MoHE) layers. MoHE routes temporal patches to a small subset of expert networks, combining a shared depthwise-convolution expert for sequence-level continuity with routed Fourier-based experts for patch-level periodic structures. MoHETS further improves robustness to non-stationary dynamics by incorporating exogenous information via cross-attention over covariate patch embeddings. Finally, we replace parameter-heavy linear projection heads with a lightweight convolutional patch decoder, improving parameter efficiency, reducing training instability, and allowing a single model to generalize across arbitrary forecast horizons. We validate across seven multivariate benchmarks and multiple horizons, with MoHETS consistently achieving state-of-the-art performance, reducing the average MSE by $12\\%$ compared to strong recent baselines, demonstrating effective heterogeneous specialization for long-term forecasting.","authors":["Evandro S. Ortigossa","Guy Lutsker","Eran Segal"],"url":"https://arxiv.org/pdf/2601.21866v1","published":"2026-01-29"}
