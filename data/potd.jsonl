{"title":"DualCast: Disentangling Aperiodic Events from Traffic Series with a Dual-Branch Model","abstract":"Traffic forecasting is an important problem in the operation and optimisation\nof transportation systems. State-of-the-art solutions train machine learning\nmodels by minimising the mean forecasting errors on the training data. The\ntrained models often favour periodic events instead of aperiodic ones in their\nprediction results, as periodic events often prevail in the training data.\nWhile offering critical optimisation opportunities, aperiodic events such as\ntraffic incidents may be missed by the existing models. To address this issue,\nwe propose DualCast -- a model framework to enhance the learning capability of\ntraffic forecasting models, especially for aperiodic events. DualCast takes a\ndual-branch architecture, to disentangle traffic signals into two types, one\nreflecting intrinsic {spatial-temporal} patterns and the other reflecting\nexternal environment contexts including aperiodic events. We further propose a\ncross-time attention mechanism, to capture high-order spatial-temporal\nrelationships from both periodic and aperiodic patterns. DualCast is versatile.\nWe integrate it with recent traffic forecasting models, consistently reducing\ntheir forecasting errors by up to 9.6% on multiple real datasets.","authors":["Xinyu Su","Feng Liu","Yanchuan Chang","Egemen Tanin","Majid Sarvi","Jianzhong Qi"],"url":"http://arxiv.org/pdf/2411.18286v1","published":"2024-11-27"}
{"title":"DualCast: Disentangling Aperiodic Events from Traffic Series with a Dual-Branch Model","abstract":"Traffic forecasting is an important problem in the operation and optimisation\nof transportation systems. State-of-the-art solutions train machine learning\nmodels by minimising the mean forecasting errors on the training data. The\ntrained models often favour periodic events instead of aperiodic ones in their\nprediction results, as periodic events often prevail in the training data.\nWhile offering critical optimisation opportunities, aperiodic events such as\ntraffic incidents may be missed by the existing models. To address this issue,\nwe propose DualCast -- a model framework to enhance the learning capability of\ntraffic forecasting models, especially for aperiodic events. DualCast takes a\ndual-branch architecture, to disentangle traffic signals into two types, one\nreflecting intrinsic {spatial-temporal} patterns and the other reflecting\nexternal environment contexts including aperiodic events. We further propose a\ncross-time attention mechanism, to capture high-order spatial-temporal\nrelationships from both periodic and aperiodic patterns. DualCast is versatile.\nWe integrate it with recent traffic forecasting models, consistently reducing\ntheir forecasting errors by up to 9.6% on multiple real datasets.","authors":["Xinyu Su","Feng Liu","Yanchuan Chang","Egemen Tanin","Majid Sarvi","Jianzhong Qi"],"url":"http://arxiv.org/pdf/2411.18286v1","published":"2024-11-27"}
{"title":"DualCast: Disentangling Aperiodic Events from Traffic Series with a Dual-Branch Model","abstract":"Traffic forecasting is an important problem in the operation and optimisation\nof transportation systems. State-of-the-art solutions train machine learning\nmodels by minimising the mean forecasting errors on the training data. The\ntrained models often favour periodic events instead of aperiodic ones in their\nprediction results, as periodic events often prevail in the training data.\nWhile offering critical optimisation opportunities, aperiodic events such as\ntraffic incidents may be missed by the existing models. To address this issue,\nwe propose DualCast -- a model framework to enhance the learning capability of\ntraffic forecasting models, especially for aperiodic events. DualCast takes a\ndual-branch architecture, to disentangle traffic signals into two types, one\nreflecting intrinsic {spatial-temporal} patterns and the other reflecting\nexternal environment contexts including aperiodic events. We further propose a\ncross-time attention mechanism, to capture high-order spatial-temporal\nrelationships from both periodic and aperiodic patterns. DualCast is versatile.\nWe integrate it with recent traffic forecasting models, consistently reducing\ntheir forecasting errors by up to 9.6% on multiple real datasets.","authors":["Xinyu Su","Feng Liu","Yanchuan Chang","Egemen Tanin","Majid Sarvi","Jianzhong Qi"],"url":"http://arxiv.org/pdf/2411.18286v1","published":"2024-11-27"}
{"title":"DualCast: Disentangling Aperiodic Events from Traffic Series with a Dual-Branch Model","abstract":"Traffic forecasting is an important problem in the operation and optimisation\nof transportation systems. State-of-the-art solutions train machine learning\nmodels by minimising the mean forecasting errors on the training data. The\ntrained models often favour periodic events instead of aperiodic ones in their\nprediction results, as periodic events often prevail in the training data.\nWhile offering critical optimisation opportunities, aperiodic events such as\ntraffic incidents may be missed by the existing models. To address this issue,\nwe propose DualCast -- a model framework to enhance the learning capability of\ntraffic forecasting models, especially for aperiodic events. DualCast takes a\ndual-branch architecture, to disentangle traffic signals into two types, one\nreflecting intrinsic {spatial-temporal} patterns and the other reflecting\nexternal environment contexts including aperiodic events. We further propose a\ncross-time attention mechanism, to capture high-order spatial-temporal\nrelationships from both periodic and aperiodic patterns. DualCast is versatile.\nWe integrate it with recent traffic forecasting models, consistently reducing\ntheir forecasting errors by up to 9.6% on multiple real datasets.","authors":["Xinyu Su","Feng Liu","Yanchuan Chang","Egemen Tanin","Majid Sarvi","Jianzhong Qi"],"url":"http://arxiv.org/pdf/2411.18286v1","published":"2024-11-27"}
{"title":"Differentiable Causal Discovery For Latent Hierarchical Causal Models","abstract":"Discovering causal structures with latent variables from observational data\nis a fundamental challenge in causal discovery. Existing methods often rely on\nconstraint-based, iterative discrete searches, limiting their scalability to\nlarge numbers of variables. Moreover, these methods frequently assume linearity\nor invertibility, restricting their applicability to real-world scenarios. We\npresent new theoretical results on the identifiability of nonlinear latent\nhierarchical causal models, relaxing previous assumptions in literature about\nthe deterministic nature of latent variables and exogenous noise. Building on\nthese insights, we develop a novel differentiable causal discovery algorithm\nthat efficiently estimates the structure of such models. To the best of our\nknowledge, this is the first work to propose a differentiable causal discovery\nmethod for nonlinear latent hierarchical models. Our approach outperforms\nexisting methods in both accuracy and scalability. We demonstrate its practical\nutility by learning interpretable hierarchical latent structures from\nhigh-dimensional image data and demonstrate its effectiveness on downstream\ntasks.","authors":["Parjanya Prashant","Ignavier Ng","Kun Zhang","Biwei Huang"],"url":"http://arxiv.org/pdf/2411.19556v1","published":"2024-11-29"}
{"title":"Differentiable Causal Discovery For Latent Hierarchical Causal Models","abstract":"Discovering causal structures with latent variables from observational data\nis a fundamental challenge in causal discovery. Existing methods often rely on\nconstraint-based, iterative discrete searches, limiting their scalability to\nlarge numbers of variables. Moreover, these methods frequently assume linearity\nor invertibility, restricting their applicability to real-world scenarios. We\npresent new theoretical results on the identifiability of nonlinear latent\nhierarchical causal models, relaxing previous assumptions in literature about\nthe deterministic nature of latent variables and exogenous noise. Building on\nthese insights, we develop a novel differentiable causal discovery algorithm\nthat efficiently estimates the structure of such models. To the best of our\nknowledge, this is the first work to propose a differentiable causal discovery\nmethod for nonlinear latent hierarchical models. Our approach outperforms\nexisting methods in both accuracy and scalability. We demonstrate its practical\nutility by learning interpretable hierarchical latent structures from\nhigh-dimensional image data and demonstrate its effectiveness on downstream\ntasks.","authors":["Parjanya Prashant","Ignavier Ng","Kun Zhang","Biwei Huang"],"url":"http://arxiv.org/pdf/2411.19556v1","published":"2024-11-29"}
{"title":"LLMForecaster: Improving Seasonal Event Forecasts with Unstructured Textual Data","abstract":"Modern time-series forecasting models often fail to make full use of rich\nunstructured information about the time series themselves. This lack of proper\nconditioning can lead to obvious model failures; for example, models may be\nunaware of the details of a particular product, and hence fail to anticipate\nseasonal surges in customer demand in the lead up to major exogenous events\nlike holidays for clearly relevant products. To address this shortcoming, this\npaper introduces a novel forecast post-processor -- which we call LLMForecaster\n-- that fine-tunes large language models (LLMs) to incorporate unstructured\nsemantic and contextual information and historical data to improve the\nforecasts from an existing demand forecasting pipeline. In an industry-scale\nretail application, we demonstrate that our technique yields statistically\nsignificantly forecast improvements across several sets of products subject to\nholiday-driven demand surges.","authors":["Hanyu Zhang","Chuck Arvin","Dmitry Efimov","Michael W. Mahoney","Dominique Perrault-Joncas","Shankar Ramasubramanian","Andrew Gordon Wilson","Malcolm Wolff"],"url":"http://arxiv.org/pdf/2412.02525v1","published":"2024-12-03"}
{"title":"Modeling and Discovering Direct Causes for Predictive Models","abstract":"We introduce a causal modeling framework that captures the input-output\nbehavior of predictive models (e.g., machine learning models) by representing\nit using causal graphs. The framework enables us to define and identify\nfeatures that directly cause the predictions, which has broad implications for\ndata collection and model evaluation. We show two assumptions under which the\ndirect causes can be discovered from data, one of which further simplifies the\ndiscovery process. In addition to providing sound and complete algorithms, we\npropose an optimization technique based on an independence rule that can be\nintegrated with the algorithms to speed up the discovery process both\ntheoretically and empirically.","authors":["Yizuo Chen","Amit Bhatia"],"url":"http://arxiv.org/pdf/2412.02878v1","published":"2024-12-03"}
{"title":"Complexity of Vector-valued Prediction: From Linear Models to Stochastic Convex Optimization","abstract":"We study the problem of learning vector-valued linear predictors: these are\nprediction rules parameterized by a matrix that maps an $m$-dimensional feature\nvector to a $k$-dimensional target. We focus on the fundamental case with a\nconvex and Lipschitz loss function, and show several new theoretical results\nthat shed light on the complexity of this problem and its connection to related\nlearning models. First, we give a tight characterization of the sample\ncomplexity of Empirical Risk Minimization (ERM) in this setting, establishing\nthat $\\smash{\\widetilde{\\Omega}}(k/\\epsilon^2)$ examples are necessary for ERM\nto reach $\\epsilon$ excess (population) risk; this provides for an exponential\nimprovement over recent results by Magen and Shamir (2023) in terms of the\ndependence on the target dimension $k$, and matches a classical upper bound due\nto Maurer (2016). Second, we present a black-box conversion from general\n$d$-dimensional Stochastic Convex Optimization (SCO) to vector-valued linear\nprediction, showing that any SCO problem can be embedded as a prediction\nproblem with $k=\\Theta(d)$ outputs. These results portray the setting of\nvector-valued linear prediction as bridging between two extensively studied yet\ndisparate learning models: linear models (corresponds to $k=1$) and general\n$d$-dimensional SCO (with $k=\\Theta(d)$).","authors":["Matan Schliserman","Tomer Koren"],"url":"http://arxiv.org/pdf/2412.04274v1","published":"2024-12-05"}
{"title":"Auto-Regressive Moving Diffusion Models for Time Series Forecasting","abstract":"Time series forecasting (TSF) is essential in various domains, and recent\nadvancements in diffusion-based TSF models have shown considerable promise.\nHowever, these models typically adopt traditional diffusion patterns, treating\nTSF as a noise-based conditional generation task. This approach neglects the\ninherent continuous sequential nature of time series, leading to a fundamental\nmisalignment between diffusion mechanisms and the TSF objective, thereby\nseverely impairing performance. To bridge this misalignment, and inspired by\nthe classic Auto-Regressive Moving Average (ARMA) theory, which views time\nseries as continuous sequential progressions evolving from previous data\npoints, we propose a novel Auto-Regressive Moving Diffusion (ARMD) model to\nfirst achieve the continuous sequential diffusion-based TSF. Unlike previous\nmethods that start from white Gaussian noise, our model employs chain-based\ndiffusion with priors, accurately modeling the evolution of time series and\nleveraging intermediate state information to improve forecasting accuracy and\nstability. Specifically, our approach reinterprets the diffusion process by\nconsidering future series as the initial state and historical series as the\nfinal state, with intermediate series generated using a sliding-based technique\nduring the forward process. This design aligns the diffusion model's sampling\nprocedure with the forecasting objective, resulting in an unconditional,\ncontinuous sequential diffusion TSF model. Extensive experiments conducted on\nseven widely used datasets demonstrate that our model achieves state-of-the-art\nperformance, significantly outperforming existing diffusion-based TSF models.\nOur code is available on GitHub: https://github.com/daxin007/ARMD.","authors":["Jiaxin Gao","Qinglong Cao","Yuntian Chen"],"url":"http://arxiv.org/pdf/2412.09328v1","published":"2024-12-12"}
{"title":"A Comprehensive Forecasting Framework based on Multi-Stage Hierarchical Forecasting Reconciliation and Adjustment","abstract":"Ads demand forecasting for Walmart's ad products plays a critical role in\nenabling effective resource planning, allocation, and management of ads\nperformance. In this paper, we introduce a comprehensive demand forecasting\nsystem that tackles hierarchical time series forecasting in business settings.\nThough traditional hierarchical reconciliation methods ensure forecasting\ncoherence, they often trade off accuracy for coherence especially at lower\nlevels and fail to capture the seasonality unique to each time-series in the\nhierarchy. Thus, we propose a novel framework \"Multi-Stage Hierarchical\nForecasting Reconciliation and Adjustment (Multi-Stage HiFoReAd)\" to address\nthe challenges of preserving seasonality, ensuring coherence, and improving\naccuracy. Our system first utilizes diverse models, ensembled through Bayesian\nOptimization (BO), achieving base forecasts. The generated base forecasts are\nthen passed into the Multi-Stage HiFoReAd framework. The initial stage refines\nthe hierarchy using Top-Down forecasts and \"harmonic alignment.\" The second\nstage aligns the higher levels' forecasts using MinTrace algorithm, following\nwhich the last two levels undergo \"harmonic alignment\" and \"stratified\nscaling\", to eventually achieve accurate and coherent forecasts across the\nwhole hierarchy. Our experiments on Walmart's internal Ads-demand dataset and 3\nother public datasets, each with 4 hierarchical levels, demonstrate that the\naverage Absolute Percentage Error from the cross-validation sets improve from\n3% to 40% across levels against BO-ensemble of models (LGBM, MSTL+ETS, Prophet)\nas well as from 1.2% to 92.9% against State-Of-The-Art models. In addition, the\nforecasts at all hierarchical levels are proved to be coherent. The proposed\nframework has been deployed and leveraged by Walmart's ads, sales and\noperations teams to track future demands, make informed decisions and plan\nresources.","authors":["Zhengchao Yang","Mithun Ghosh","Anish Saha","Dong Xu","Konstantin Shmakov","Kuang-chih Lee"],"url":"http://arxiv.org/pdf/2412.14718v1","published":"2024-12-19"}
{"title":"Neural Conformal Control for Time Series Forecasting","abstract":"We introduce a neural network conformal prediction method for time series\nthat enhances adaptivity in non-stationary environments. Our approach acts as a\nneural controller designed to achieve desired target coverage, leveraging\nauxiliary multi-view data with neural network encoders in an end-to-end manner\nto further enhance adaptivity. Additionally, our model is designed to enhance\nthe consistency of prediction intervals in different quantiles by integrating\nmonotonicity constraints and leverages data from related tasks to boost\nfew-shot learning performance. Using real-world datasets from epidemics,\nelectric demand, weather, and others, we empirically demonstrate significant\nimprovements in coverage and probabilistic accuracy, and find that our method\nis the only one that combines good calibration with consistency in prediction\nintervals.","authors":["Ruipu Li","Alexander Rodr\u00edguez"],"url":"http://arxiv.org/pdf/2412.18144v1","published":"2024-12-24"}
{"title":"TimeRAF: Retrieval-Augmented Foundation model for Zero-shot Time Series Forecasting","abstract":"Time series forecasting plays a crucial role in data mining, driving rapid\nadvancements across numerous industries. With the emergence of large models,\ntime series foundation models (TSFMs) have exhibited remarkable generalization\ncapabilities, such as zero-shot learning, through large-scale pre-training.\nMeanwhile, Retrieval-Augmented Generation (RAG) methods have been widely\nemployed to enhance the performance of foundation models on unseen data,\nallowing models to access to external knowledge. In this paper, we introduce\nTimeRAF, a Retrieval-Augmented Forecasting model that enhance zero-shot time\nseries forecasting through retrieval-augmented techniques. We develop\ncustomized time series knowledge bases that are tailored to the specific\nforecasting tasks. TimeRAF employs an end-to-end learnable retriever to extract\nvaluable information from the knowledge base. Additionally, we propose Channel\nPrompting for knowledge integration, which effectively extracts relevant\ninformation from the retrieved knowledge along the channel dimension. Extensive\nexperiments demonstrate the effectiveness of our model, showing significant\nimprovement across various domains and datasets.","authors":["Huanyu Zhang","Chang Xu","Yi-Fan Zhang","Zhang Zhang","Liang Wang","Jiang Bian","Tieniu Tan"],"url":"http://arxiv.org/pdf/2412.20810v1","published":"2024-12-30"}
{"title":"Battling the Non-stationarity in Time Series Forecasting via Test-time Adaptation","abstract":"Deep Neural Networks have spearheaded remarkable advancements in time series\nforecasting (TSF), one of the major tasks in time series modeling. Nonetheless,\nthe non-stationarity of time series undermines the reliability of pre-trained\nsource time series forecasters in mission-critical deployment settings. In this\nstudy, we introduce a pioneering test-time adaptation framework tailored for\nTSF (TSF-TTA). TAFAS, the proposed approach to TSF-TTA, flexibly adapts source\nforecasters to continuously shifting test distributions while preserving the\ncore semantic information learned during pre-training. The novel utilization of\npartially-observed ground truth and gated calibration module enables proactive,\nrobust, and model-agnostic adaptation of source forecasters. Experiments on\ndiverse benchmark datasets and cutting-edge architectures demonstrate the\nefficacy and generality of TAFAS, especially in long-term forecasting scenarios\nthat suffer from significant distribution shifts. The code is available at\nhttps://github.com/kimanki/TAFAS.","authors":["HyunGi Kim","Siwon Kim","Jisoo Mok","Sungroh Yoon"],"url":"http://arxiv.org/pdf/2501.04970v1","published":"2025-01-09"}
{"title":"Kolmogorov-Arnold Networks for Time Series Granger Causality Inference","abstract":"We introduce Granger Causality Kolmogorov-Arnold Networks (GCKAN), an\ninnovative architecture that extends the recently proposed Kolmogorov-Arnold\nNetworks (KAN) to the domain of causal inference. By extracting base weights\nfrom KAN layers and incorporating the sparsity-inducing penalty along with\nridge regularization, GCKAN infers the Granger causality from time series while\nenabling automatic time lag selection. Additionally, we propose an algorithm\nleveraging time-reversed Granger causality to enhance inference accuracy. The\nalgorithm compares prediction and sparse-inducing losses derived from the\noriginal and time-reversed series, automatically selecting the casual\nrelationship with the higher score or integrating the results to mitigate\nspurious connectivities. Comprehensive experiments conducted on Lorenz-96, gene\nregulatory networks, fMRI BOLD signals, and VAR datasets demonstrate that the\nproposed model achieves competitive performance to state-of-the-art methods in\ninferring Granger causality from nonlinear, high-dimensional, and\nlimited-sample time series.","authors":["Meiliang Liu","Yunfang Xu","Zijin Li","Zhengye Si","Xiaoxiao Yang","Xinyue Yang","Zhiwen Zhao"],"url":"http://arxiv.org/pdf/2501.08958v1","published":"2025-01-15"}
{"title":"GCAD: Anomaly Detection in Multivariate Time Series from the Perspective of Granger Causality","abstract":"Multivariate time series anomaly detection has numerous real-world\napplications and is being extensively studied. Modeling pairwise correlations\nbetween variables is crucial. Existing methods employ learnable graph\nstructures and graph neural networks to explicitly model the spatial\ndependencies between variables. However, these methods are primarily based on\nprediction or reconstruction tasks, which can only learn similarity\nrelationships between sequence embeddings and lack interpretability in how\ngraph structures affect time series evolution. In this paper, we designed a\nframework that models spatial dependencies using interpretable causal\nrelationships and detects anomalies through changes in causal patterns.\nSpecifically, we propose a method to dynamically discover Granger causality\nusing gradients in nonlinear deep predictors and employ a simple sparsification\nstrategy to obtain a Granger causality graph, detecting anomalies from a causal\nperspective. Experiments on real-world datasets demonstrate that the proposed\nmodel achieves more accurate anomaly detection compared to baseline methods.","authors":["Zehao Liu","Mengzhou Gao","Pengfei Jiao"],"url":"http://arxiv.org/pdf/2501.13493v1","published":"2025-01-23"}
{"title":"Prediction-Powered Inference with Imputed Covariates and Nonuniform Sampling","abstract":"Machine learning models are increasingly used to produce predictions that\nserve as input data in subsequent statistical analyses. For example, computer\nvision predictions of economic and environmental indicators based on satellite\nimagery are used in downstream regressions; similarly, language models are\nwidely used to approximate human ratings and opinions in social science\nresearch. However, failure to properly account for errors in the machine\nlearning predictions renders standard statistical procedures invalid. Prior\nwork uses what we call the Predict-Then-Debias estimator to give valid\nconfidence intervals when machine learning algorithms impute missing variables,\nassuming a small complete sample from the population of interest. We expand the\nscope by introducing bootstrap confidence intervals that apply when the\ncomplete data is a nonuniform (i.e., weighted, stratified, or clustered) sample\nand to settings where an arbitrary subset of features is imputed. Importantly,\nthe method can be applied to many settings without requiring additional\ncalculations. We prove that these confidence intervals are valid under no\nassumptions on the quality of the machine learning model and are no wider than\nthe intervals obtained by methods that do not use machine learning predictions.","authors":["Dan M. Kluger","Kerri Lu","Tijana Zrnic","Sherrie Wang","Stephen Bates"],"url":"http://arxiv.org/pdf/2501.18577v1","published":"2025-01-30"}
{"title":"On the importance of structural identifiability for machine learning with partially observed dynamical systems","abstract":"The successful application of modern machine learning for time series\nclassification is often hampered by limitations in quality and quantity of\navailable training data. To overcome these limitations, available domain expert\nknowledge in the form of parametrised mechanistic dynamical models can be used\nwhenever it is available and time series observations may be represented as an\nelement from a given class of parametrised dynamical models. This makes the\nlearning process interpretable and allows the modeller to deal with sparsely\nand irregularly sampled data in a natural way. However, the internal processes\nof a dynamical model are often only partially observed. This can lead to\nambiguity regarding which particular model realization best explains a given\ntime series observation. This problem is well-known in the literature, and a\ndynamical model with this issue is referred to as structurally unidentifiable.\nTraining a classifier that incorporates knowledge about a structurally\nunidentifiable dynamical model can negatively influence classification\nperformance. To address this issue, we employ structural identifiability\nanalysis to explicitly relate parameter configurations that are associated with\nidentical system outputs. Using the derived relations in classifier training,\nwe demonstrate that this method significantly improves the classifier's ability\nto generalize to unseen data on a number of example models from the biomedical\ndomain. This effect is especially pronounced when the number of training\ninstances is limited. Our results demonstrate the importance of accounting for\nstructural identifiability, a topic that has received relatively little\nattention from the machine learning community.","authors":["Janis Norden","Elisa Oostwal","Michael Chappell","Peter Tino","Kerstin Bunte"],"url":"http://arxiv.org/pdf/2502.04131v1","published":"2025-02-06"}
{"title":"Relational Conformal Prediction for Correlated Time Series","abstract":"We address the problem of uncertainty quantification in time series\nforecasting by exploiting observations at correlated sequences. Relational deep\nlearning methods leveraging graph representations are among the most effective\ntools for obtaining point estimates from spatiotemporal data and correlated\ntime series. However, the problem of exploiting relational structures to\nestimate the uncertainty of such predictions has been largely overlooked in the\nsame context. To this end, we propose a novel distribution-free approach based\non the conformal prediction framework and quantile regression. Despite the\nrecent applications of conformal prediction to sequential data, existing\nmethods operate independently on each target time series and do not account for\nrelationships among them when constructing the prediction interval. We fill\nthis void by introducing a novel conformal prediction method based on graph\ndeep learning operators. Our method, named Conformal Relational Prediction\n(CoRel), does not require the relational structure (graph) to be known as a\nprior and can be applied on top of any pre-trained time series predictor.\nAdditionally, CoRel includes an adaptive component to handle non-exchangeable\ndata and changes in the input time series. Our approach provides accurate\ncoverage and archives state-of-the-art uncertainty quantification in relevant\nbenchmarks.","authors":["Andrea Cini","Alexander Jenkins","Danilo Mandic","Cesare Alippi","Filippo Maria Bianchi"],"url":"http://arxiv.org/pdf/2502.09443v1","published":"2025-02-13"}
{"title":"Not All Data are Good Labels: On the Self-supervised Labeling for Time Series Forecasting","abstract":"Time Series Forecasting (TSF) is a crucial task in various domains, yet\nexisting TSF models rely heavily on high-quality data and insufficiently\nexploit all available data. This paper explores a novel self-supervised\napproach to re-label time series datasets by inherently constructing candidate\ndatasets. During the optimization of a simple reconstruction network,\nintermediates are used as pseudo labels in a self-supervised paradigm,\nimproving generalization for any predictor. We introduce the Self-Correction\nwith Adaptive Mask (SCAM), which discards overfitted components and selectively\nreplaces them with pseudo labels generated from reconstructions. Additionally,\nwe incorporate Spectral Norm Regularization (SNR) to further suppress\noverfitting from a loss landscape perspective. Our experiments on eleven\nreal-world datasets demonstrate that SCAM consistently improves the performance\nof various backbone models. This work offers a new perspective on constructing\ndatasets and enhancing the generalization of TSF models through self-supervised\nlearning.","authors":["Yuxuan Yang","Dalin Zhang","Yuxuan Liang","Hua Lu","Huan Li","Gang Chen"],"url":"http://arxiv.org/pdf/2502.14704v1","published":"2025-02-20"}
{"title":"Efficient Time Series Forecasting via Hyper-Complex Models and Frequency Aggregation","abstract":"Time series forecasting is a long-standing problem in statistics and machine\nlearning. One of the key challenges is processing sequences with long-range\ndependencies. To that end, a recent line of work applied the short-time Fourier\ntransform (STFT), which partitions the sequence into multiple subsequences and\napplies a Fourier transform to each separately. We propose the Frequency\nInformation Aggregation (FIA)-Net, which is based on a novel complex-valued MLP\narchitecture that aggregates adjacent window information in the frequency\ndomain. To further increase the receptive field of the FIA-Net, we treat the\nset of windows as hyper-complex (HC) valued vectors and employ HC algebra to\nefficiently combine information from all STFT windows altogether. Using the\nHC-MLP backbone allows for improved handling of sequences with long-term\ndependence. Furthermore, due to the nature of HC operations, the HC-MLP uses up\nto three times fewer parameters than the equivalent standard window aggregation\nmethod. We evaluate the FIA-Net on various time-series benchmarks and show that\nthe proposed methodologies outperform existing state of the art methods in\nterms of both accuracy and efficiency. Our code is publicly available on\nhttps://anonymous.4open.science/r/research-1803/.","authors":["Eyal Yakir","Dor Tsur","Haim Permuter"],"url":"http://arxiv.org/pdf/2502.19983v1","published":"2025-02-27"}
{"title":"TimeFound: A Foundation Model for Time Series Forecasting","abstract":"We present TimeFound, an encoder-decoder transformer-based time series\nfoundation model for out-of-the-box zero-shot forecasting. To handle time\nseries data from various domains, TimeFound employs a multi-resolution patching\nstrategy to capture complex temporal patterns at multiple scales. We pre-train\nour model with two sizes (200M and 710M parameters) on a large time-series\ncorpus comprising both real-world and synthetic datasets. Over a collection of\nunseen datasets across diverse domains and forecasting horizons, our empirical\nevaluations suggest that TimeFound can achieve superior or competitive\nzero-shot forecasting performance, compared to state-of-the-art time series\nfoundation models.","authors":["Congxi Xiao","Jingbo Zhou","Yixiong Xiao","Xinjiang Lu","Le Zhang","Hui Xiong"],"url":"http://arxiv.org/pdf/2503.04118v1","published":"2025-03-06"}
{"title":"Deep Learning for Time Series Forecasting: A Survey","abstract":"Time series forecasting (TSF) has long been a crucial task in both industry\nand daily life. Most classical statistical models may have certain limitations\nwhen applied to practical scenarios in fields such as energy, healthcare,\ntraffic, meteorology, and economics, especially when high accuracy is required.\nWith the continuous development of deep learning, numerous new models have\nemerged in the field of time series forecasting in recent years. However,\nexisting surveys have not provided a unified summary of the wide range of model\narchitectures in this field, nor have they given detailed summaries of works in\nfeature extraction and datasets. To address this gap, in this review, we\ncomprehensively study the previous works and summarize the general paradigms of\nDeep Time Series Forecasting (DTSF) in terms of model architectures. Besides,\nwe take an innovative approach by focusing on the composition of time series\nand systematically explain important feature extraction methods. Additionally,\nwe provide an overall compilation of datasets from various domains in existing\nworks. Finally, we systematically emphasize the significant challenges faced\nand future research directions in this field.","authors":["Xiangjie Kong","Zhenghao Chen","Weiyao Liu","Kaili Ning","Lechao Zhang","Syauqie Muhammad Marier","Yichen Liu","Yuhao Chen","Feng Xia"],"url":"http://arxiv.org/pdf/2503.10198v1","published":"2025-03-13"}
{"title":"Truthful Elicitation of Imprecise Forecasts","abstract":"The quality of probabilistic forecasts is crucial for decision-making under\nuncertainty. While proper scoring rules incentivize truthful reporting of\nprecise forecasts, they fall short when forecasters face epistemic uncertainty\nabout their beliefs, limiting their use in safety-critical domains where\ndecision-makers (DMs) prioritize proper uncertainty management. To address\nthis, we propose a framework for scoring imprecise forecasts -- forecasts given\nas a set of beliefs. Despite existing impossibility results for deterministic\nscoring rules, we enable truthful elicitation by drawing connection to social\nchoice theory and introducing a two-way communication framework where DMs first\nshare their aggregation rules (e.g., averaging or min-max) used in downstream\ndecisions for resolving forecast ambiguity. This, in turn, helps forecasters\nresolve indecision during elicitation. We further show that truthful\nelicitation of imprecise forecasts is achievable using proper scoring rules\nrandomized over the aggregation procedure. Our approach allows DM to elicit and\nintegrate the forecaster's epistemic uncertainty into their decision-making\nprocess, thus improving credibility.","authors":["Anurag Singh","Siu Lun Chau","Krikamol Muandet"],"url":"http://arxiv.org/pdf/2503.16395v1","published":"2025-03-20"}
{"title":"Interpretable Cross-Sphere Multiscale Deep Learning Predicts ENSO Skilfully Beyond 2 Years","abstract":"El Ni\\~no-Southern Oscillation (ENSO) exerts global climate and societal\nimpacts, but real-time prediction with lead times beyond one year remains\nchallenging. Dynamical models suffer from large biases and uncertainties, while\ndeep learning struggles with interpretability and multi-scale dynamics. Here,\nwe introduce PTSTnet, an interpretable model that unifies dynamical processes\nand cross-scale spatiotemporal learning in an innovative neural-network\nframework with physics-encoding learning. PTSTnet produces interpretable\npredictions significantly outperforming state-of-the-art benchmarks with lead\ntimes beyond 24 months, providing physical insights into error propagation in\nocean-atmosphere interactions. PTSTnet learns feature representations with\nphysical consistency from sparse data to tackle inherent multi-scale and\nmulti-physics challenges underlying ocean-atmosphere processes, thereby\ninherently enhancing long-term prediction skill. Our successful realizations\nmark substantial steps forward in interpretable insights into innovative neural\nocean modelling.","authors":["Rixu Hao","Yuxin Zhao","Shaoqing Zhang","Guihua Wang","Xiong Deng"],"url":"http://arxiv.org/pdf/2503.21211v1","published":"2025-03-27"}
{"title":"Online Multivariate Regularized Distributional Regression for High-dimensional Probabilistic Electricity Price Forecasting","abstract":"Probabilistic electricity price forecasting (PEPF) is a key task for market\nparticipants in short-term electricity markets. The increasing availability of\nhigh-frequency data and the need for real-time decision-making in energy\nmarkets require online estimation methods for efficient model updating. We\npresent an online, multivariate, regularized distributional regression model,\nallowing for the modeling of all distribution parameters conditional on\nexplanatory variables. Our approach is based on the combination of the\nmultivariate distributional regression and an efficient online learning\nalgorithm based on online coordinate descent for LASSO-type regularization.\nAdditionally, we propose to regularize the estimation along a path of\nincreasingly complex dependence structures of the multivariate distribution,\nallowing for parsimonious estimation and early stopping. We validate our\napproach through one of the first forecasting studies focusing on multivariate\nprobabilistic forecasting in the German day-ahead electricity market while\nusing only online estimation methods. We compare our approach to online\nLASSO-ARX-models with adaptive marginal distribution and to online univariate\ndistributional models combined with an adaptive Copula. We show that the\nmultivariate distributional regression, which allows modeling all distribution\nparameters - including the mean and the dependence structure - conditional on\nexplanatory variables such as renewable in-feed or past prices provide superior\nforecasting performance compared to modeling of the marginals only and keeping\na static/unconditional dependence structure. Additionally, online estimation\nyields a speed-up by a factor of 80 to over 400 times compared to batch\nfitting.","authors":["Simon Hirsch"],"url":"http://arxiv.org/pdf/2504.02518v1","published":"2025-04-03"}
{"title":"Deep Learning Meets Teleconnections: Improving S2S Predictions for European Winter Weather","abstract":"Predictions on subseasonal-to-seasonal (S2S) timescales--ranging from two\nweeks to two month--are crucial for early warning systems but remain\nchallenging owing to chaos in the climate system. Teleconnections, such as the\nstratospheric polar vortex (SPV) and Madden-Julian Oscillation (MJO), offer\nwindows of enhanced predictability, however, their complex interactions remain\nunderutilized in operational forecasting. Here, we developed and evaluated deep\nlearning architectures to predict North Atlantic-European (NAE) weather\nregimes, systematically assessing the role of remote drivers in improving S2S\nforecast skill of deep learning models. We implemented (1) a Long Short-term\nMemory (LSTM) network predicting the NAE regimes of the next six weeks based on\nprevious regimes, (2) an Index-LSTM incorporating SPV and MJO indices, and (3)\na ViT-LSTM using a Vision Transformer to directly encode stratospheric wind and\ntropical outgoing longwave radiation fields. These models are compared with\noperational hindcasts as well as other AI models. Our results show that\nleveraging teleconnection information enhances skill at longer lead times.\nNotably, the ViT-LSTM outperforms ECMWF's subseasonal hindcasts beyond week 4\nby improving Scandinavian Blocking (SB) and Atlantic Ridge (AR) predictions.\nAnalysis of high-confidence predictions reveals that NAO-, SB, and AR\nopportunity forecasts can be associated with SPV variability and MJO phase\npatterns aligning with established pathways, also indicating new patterns.\nOverall, our work demonstrates that encoding physically meaningful climate\nfields can enhance S2S prediction skill, advancing AI-driven subseasonal\nforecast. Moreover, the experiments highlight the potential of deep learning\nmethods as investigative tools, providing new insights into atmospheric\ndynamics and predictability.","authors":["Philine L. Bommer","Marlene Kretschmer","Fiona R. Spuler","Kirill Bykov","Marina M. -C. H\u00f6hne"],"url":"http://arxiv.org/pdf/2504.07625v1","published":"2025-04-10"}
{"title":"Fine Flood Forecasts: Incorporating local data into global models through fine-tuning","abstract":"Floods are the most common form of natural disaster and accurate flood\nforecasting is essential for early warning systems. Previous work has shown\nthat machine learning (ML) models are a promising way to improve flood\npredictions when trained on large, geographically-diverse datasets. This\nrequirement of global training can result in a loss of ownership for national\nforecasters who cannot easily adapt the models to improve performance in their\nregion, preventing ML models from being operationally deployed. Furthermore,\ntraditional hydrology research with physics-based models suggests that local\ndata -- which in many cases is only accessible to local agencies -- is valuable\nfor improving model performance. To address these concerns, we demonstrate a\nmethodology of pre-training a model on a large, global dataset and then\nfine-tuning that model on data from individual basins. This results in\nperformance increases, validating our hypothesis that there is extra\ninformation to be captured in local data. In particular, we show that\nperformance increases are most significant in watersheds that underperform\nduring global training. We provide a roadmap for national forecasters who wish\nto take ownership of global models using their own data, aiming to lower the\nbarrier to operational deployment of ML-based hydrological forecast systems.","authors":["Emil Ryd","Grey Nearing"],"url":"http://arxiv.org/pdf/2504.12559v1","published":"2025-04-17"}
{"title":"Scalable Permutation-Aware Modeling for Temporal Set Prediction","abstract":"Temporal set prediction involves forecasting the elements that will appear in\nthe next set, given a sequence of prior sets, each containing a variable number\nof elements. Existing methods often rely on intricate architectures with\nsubstantial computational overhead, which hampers their scalability. In this\nwork, we introduce a novel and scalable framework that leverages\npermutation-equivariant and permutation-invariant transformations to\nefficiently model set dynamics. Our approach significantly reduces both\ntraining and inference time while maintaining competitive performance.\nExtensive experiments on multiple public benchmarks show that our method\nachieves results on par with or superior to state-of-the-art models across\nseveral evaluation metrics. These results underscore the effectiveness of our\nmodel in enabling efficient and scalable temporal set prediction.","authors":["Ashish Ranjan","Ayush Agarwal","Shalin Barot","Sushant Kumar"],"url":"http://arxiv.org/pdf/2504.17140v1","published":"2025-04-23"}
{"title":"Unlocking the Potential of Linear Networks for Irregular Multivariate Time Series Forecasting","abstract":"Time series forecasting holds significant importance across various\nindustries, including finance, transportation, energy, healthcare, and climate.\nDespite the widespread use of linear networks due to their low computational\ncost and effectiveness in modeling temporal dependencies, most existing\nresearch has concentrated on regularly sampled and fully observed multivariate\ntime series. However, in practice, we frequently encounter irregular\nmultivariate time series characterized by variable sampling intervals and\nmissing values. The inherent intra-series inconsistency and inter-series\nasynchrony in such data hinder effective modeling and forecasting with\ntraditional linear networks relying on static weights. To tackle these\nchallenges, this paper introduces a novel model named AiT. AiT utilizes an\nadaptive linear network capable of dynamically adjusting weights according to\nobservation time points to address intra-series inconsistency, thereby\nenhancing the accuracy of temporal dependencies modeling. Furthermore, by\nincorporating the Transformer module on variable semantics embeddings, AiT\nefficiently captures variable correlations, avoiding the challenge of\ninter-series asynchrony. Comprehensive experiments across four benchmark\ndatasets demonstrate the superiority of AiT, improving prediction accuracy by\n11% and decreasing runtime by 52% compared to existing state-of-the-art\nmethods.","authors":["Chengsen Wang","Qi Qi","Jingyu Wang","Haifeng Sun","Zirui Zhuang","Jianxin Liao"],"url":"http://arxiv.org/pdf/2505.00590v1","published":"2025-05-01"}
{"title":"Long-Term Individual Causal Effect Estimation via Identifiable Latent Representation Learning","abstract":"Estimating long-term causal effects by combining long-term observational and\nshort-term experimental data is a crucial but challenging problem in many\nreal-world scenarios. In existing methods, several ideal assumptions, e.g.\nlatent unconfoundedness assumption or additive equi-confounding bias\nassumption, are proposed to address the latent confounder problem raised by the\nobservational data. However, in real-world applications, these assumptions are\ntypically violated which limits their practical effectiveness. In this paper,\nwe tackle the problem of estimating the long-term individual causal effects\nwithout the aforementioned assumptions. Specifically, we propose to utilize the\nnatural heterogeneity of data, such as data from multiple sources, to identify\nlatent confounders, thereby significantly avoiding reliance on idealized\nassumptions. Practically, we devise a latent representation learning-based\nestimator of long-term causal effects. Theoretically, we establish the\nidentifiability of latent confounders, with which we further achieve long-term\neffect identification. Extensive experimental studies, conducted on multiple\nsynthetic and semi-synthetic datasets, demonstrate the effectiveness of our\nproposed method.","authors":["Ruichu Cai","Junjie Wan","Weilin Chen","Zeqin Yang","Zijian Li","Peng Zhen","Jiecheng Guo"],"url":"http://arxiv.org/pdf/2505.05192v1","published":"2025-05-08"}
{"title":"ChronoSteer: Bridging Large Language Model and Time Series Foundation Model via Synthetic Data","abstract":"Conventional forecasting methods rely on unimodal time series data, limiting\ntheir ability to exploit rich textual information. Recently, large language\nmodels (LLMs) and time series foundation models (TSFMs) have demonstrated\npowerful capability in textual reasoning and temporal modeling, respectively.\nIntegrating the strengths of both to construct a multimodal model that\nconcurrently leverages both temporal and textual information for future\ninference has emerged as a critical research challenge. To address the scarcity\nof event-series paired data, we propose a decoupled framework: an LLM is\nemployed to transform textual events into revision instructions, which are then\nused to steer the output of TSFM. To implement this framework, we introduce\nChronoSteer, a multimodal TSFM that can be steered through textual revision\ninstructions, effectively bridging LLM and TSFM. Moreover, to mitigate the\nshortage of cross-modal instruction-series paired data, we devise a two-stage\ntraining strategy based on synthetic data. In addition, we also construct a\nhigh-quality multimodal time series forecasting benchmark to address the\ninformation leakage concerns during evaluation. After integrating with an LLM,\nChronoSteer, which is trained exclusively on synthetic data, achieves a 25.7%\nimprovement in prediction accuracy compared to the unimodal backbone and a\n22.5% gain over the previous state-of-the-art multimodal method.","authors":["Chengsen Wang","Qi Qi","Zhongwen Rao","Lujia Pan","Jingyu Wang","Jianxin Liao"],"url":"http://arxiv.org/pdf/2505.10083v1","published":"2025-05-15"}
{"title":"CausalDynamics: A large-scale benchmark for structural discovery of dynamical causal models","abstract":"Causal discovery for dynamical systems poses a major challenge in fields\nwhere active interventions are infeasible. Most methods used to investigate\nthese systems and their associated benchmarks are tailored to deterministic,\nlow-dimensional and weakly nonlinear time-series data. To address these\nlimitations, we present CausalDynamics, a large-scale benchmark and extensible\ndata generation framework to advance the structural discovery of dynamical\ncausal models. Our benchmark consists of true causal graphs derived from\nthousands of coupled ordinary and stochastic differential equations as well as\ntwo idealized climate models. We perform a comprehensive evaluation of\nstate-of-the-art causal discovery algorithms for graph reconstruction on\nsystems with noisy, confounded, and lagged dynamics. CausalDynamics consists of\na plug-and-play, build-your-own coupling workflow that enables the construction\nof a hierarchy of physical systems. We anticipate that our framework will\nfacilitate the development of robust causal discovery algorithms that are\nbroadly applicable across domains while addressing their unique challenges. We\nprovide a user-friendly implementation and documentation on\nhttps://kausable.github.io/CausalDynamics.","authors":["Benjamin Herdeanu","Juan Nathaniel","Carla Roesch","Jatan Buch","Gregor Ramien","Johannes Haux","Pierre Gentine"],"url":"http://arxiv.org/pdf/2505.16620v1","published":"2025-05-22"}
{"title":"Improving Time Series Forecasting via Instance-aware Post-hoc Revision","abstract":"Time series forecasting plays a vital role in various real-world applications\nand has attracted significant attention in recent decades. While recent methods\nhave achieved remarkable accuracy by incorporating advanced inductive biases\nand training strategies, we observe that instance-level variations remain a\nsignificant challenge. These variations--stemming from distribution shifts,\nmissing data, and long-tail patterns--often lead to suboptimal forecasts for\nspecific instances, even when overall performance appears strong. To address\nthis issue, we propose a model-agnostic framework, PIR, designed to enhance\nforecasting performance through Post-forecasting Identification and Revision.\nSpecifically, PIR first identifies biased forecasting instances by estimating\ntheir accuracy. Based on this, the framework revises the forecasts using\ncontextual information, including covariates and historical time series, from\nboth local and global perspectives in a post-processing fashion. Extensive\nexperiments on real-world datasets with mainstream forecasting models\ndemonstrate that PIR effectively mitigates instance-level errors and\nsignificantly improves forecasting reliability.","authors":["Zhiding Liu","Mingyue Cheng","Guanhao Zhao","Jiqian Yang","Qi Liu","Enhong Chen"],"url":"http://arxiv.org/pdf/2505.23583v1","published":"2025-05-29"}
{"title":"Nonlinear Causal Discovery for Grouped Data","abstract":"Inferring cause-effect relationships from observational data has gained\nsignificant attention in recent years, but most methods are limited to scalar\nrandom variables. In many important domains, including neuroscience,\npsychology, social science, and industrial manufacturing, the causal units of\ninterest are groups of variables rather than individual scalar measurements.\nMotivated by these applications, we extend nonlinear additive noise models to\nhandle random vectors, establishing a two-step approach for causal graph\nlearning: First, infer the causal order among random vectors. Second, perform\nmodel selection to identify the best graph consistent with this order. We\nintroduce effective and novel solutions for both steps in the vector case,\ndemonstrating strong performance in simulations. Finally, we apply our method\nto real-world assembly line data with partial knowledge of causal ordering\namong variable groups.","authors":["Konstantin G\u00f6bler","Tobias Windisch","Mathias Drton"],"url":"http://arxiv.org/pdf/2506.05120v1","published":"2025-06-05"}
{"title":"Time-IMM: A Dataset and Benchmark for Irregular Multimodal Multivariate Time Series","abstract":"Time series data in real-world applications such as healthcare, climate\nmodeling, and finance are often irregular, multimodal, and messy, with varying\nsampling rates, asynchronous modalities, and pervasive missingness. However,\nexisting benchmarks typically assume clean, regularly sampled, unimodal data,\ncreating a significant gap between research and real-world deployment. We\nintroduce Time-IMM, a dataset specifically designed to capture cause-driven\nirregularity in multimodal multivariate time series. Time-IMM represents nine\ndistinct types of time series irregularity, categorized into trigger-based,\nconstraint-based, and artifact-based mechanisms. Complementing the dataset, we\nintroduce IMM-TSF, a benchmark library for forecasting on irregular multimodal\ntime series, enabling asynchronous integration and realistic evaluation.\nIMM-TSF includes specialized fusion modules, including a timestamp-to-text\nfusion module and a multimodality fusion module, which support both\nrecency-aware averaging and attention-based integration strategies. Empirical\nresults demonstrate that explicitly modeling multimodality on irregular time\nseries data leads to substantial gains in forecasting performance. Time-IMM and\nIMM-TSF provide a foundation for advancing time series analysis under\nreal-world conditions. The dataset is publicly available at\nhttps://www.kaggle.com/datasets/blacksnail789521/time-imm/data, and the\nbenchmark library can be accessed at\nhttps://anonymous.4open.science/r/IMMTSF_NeurIPS2025.","authors":["Ching Chang","Jeehyun Hwang","Yidan Shi","Haixin Wang","Wen-Chih Peng","Tien-Fu Chen","Wei Wang"],"url":"http://arxiv.org/pdf/2506.10412v1","published":"2025-06-12"}
{"title":"Warping and Matching Subsequences Between Time Series","abstract":"Comparing time series is essential in various tasks such as clustering and\nclassification. While elastic distance measures that allow warping provide a\nrobust quantitative comparison, a qualitative comparison on top of them is\nmissing. Traditional visualizations focus on point-to-point alignment and do\nnot convey the broader structural relationships at the level of subsequences.\nThis limitation makes it difficult to understand how and where one time series\nshifts, speeds up or slows down with respect to another. To address this, we\npropose a novel technique that simplifies the warping path to highlight,\nquantify and visualize key transformations (shift, compression, difference in\namplitude). By offering a clearer representation of how subsequences match\nbetween time series, our method enhances interpretability in time series\ncomparison.","authors":["Simiao Lin","Wannes Meert","Pieter Robberechts","Hendrik Blockeel"],"url":"http://arxiv.org/pdf/2506.15452v1","published":"2025-06-18"}
{"title":"Graph-Structured Feedback Multimodel Ensemble Online Conformal Prediction","abstract":"Online conformal prediction has demonstrated its capability to construct a\nprediction set for each incoming data point that covers the true label with a\npredetermined probability. To cope with potential distribution shift,\nmulti-model online conformal prediction has been introduced to select and\nleverage different models from a preselected candidate set. Along with the\nimproved flexibility, the choice of the preselected set also brings challenges.\nA candidate set that includes a large number of models may increase the\ncomputational complexity. In addition, the inclusion of irrelevant models with\npoor performance may negatively impact the performance and lead to\nunnecessarily large prediction sets. To address these challenges, we propose a\nnovel multi-model online conformal prediction algorithm that identifies a\nsubset of effective models at each time step by collecting feedback from a\nbipartite graph, which is refined upon receiving new data. A model is then\nselected from this subset to construct the prediction set, resulting in reduced\ncomputational complexity and smaller prediction sets. Additionally, we\ndemonstrate that using prediction set size as feedback, alongside model loss,\ncan significantly improve efficiency by constructing smaller prediction sets\nwhile still satisfying the required coverage guarantee. The proposed algorithms\nare proven to ensure valid coverage and achieve sublinear regret. Experiments\non real and synthetic datasets validate that the proposed methods construct\nsmaller prediction sets and outperform existing multi-model online conformal\nprediction approaches.","authors":["Erfan Hajihashemi","Yanning Shen"],"url":"http://arxiv.org/pdf/2506.20898v1","published":"2025-06-26"}
{"title":"Statistical Inference for Responsiveness Verification","abstract":"Many safety failures in machine learning arise when models are used to assign\npredictions to people (often in settings like lending, hiring, or content\nmoderation) without accounting for how individuals can change their inputs. In\nthis work, we introduce a formal validation procedure for the responsiveness of\npredictions with respect to interventions on their features. Our procedure\nframes responsiveness as a type of sensitivity analysis in which practitioners\ncontrol a set of changes by specifying constraints over interventions and\ndistributions over downstream effects. We describe how to estimate\nresponsiveness for the predictions of any model and any dataset using only\nblack-box access, and how to use these estimates to support tasks such as\nfalsification and failure probability estimation. We develop algorithms that\nconstruct these estimates by generating a uniform sample of reachable points,\nand demonstrate how they can promote safety in real-world applications such as\nrecidivism prediction, organ transplant prioritization, and content moderation.","authors":["Seung Hyun Cheon","Meredith Stewart","Bogdan Kulynych","Tsui-Wei Weng","Berk Ustun"],"url":"http://arxiv.org/pdf/2507.02169v1","published":"2025-07-02"}
{"title":"Efficient Causal Discovery for Autoregressive Time Series","abstract":"In this study, we present a novel constraint-based algorithm for causal\nstructure learning specifically designed for nonlinear autoregressive time\nseries. Our algorithm significantly reduces computational complexity compared\nto existing methods, making it more efficient and scalable to larger problems.\nWe rigorously evaluate its performance on synthetic datasets, demonstrating\nthat our algorithm not only outperforms current techniques, but also excels in\nscenarios with limited data availability. These results highlight its potential\nfor practical applications in fields requiring efficient and accurate causal\ninference from nonlinear time series data.","authors":["Mohammad Fesanghary","Achintya Gopal"],"url":"http://arxiv.org/pdf/2507.07898v1","published":"2025-07-10"}
{"title":"MoTM: Towards a Foundation Model for Time Series Imputation based on Continuous Modeling","abstract":"Recent years have witnessed a growing interest for time series foundation\nmodels, with a strong emphasis on the forecasting task. Yet, the crucial task\nof out-of-domain imputation of missing values remains largely underexplored. We\npropose a first step to fill this gap by leveraging implicit neural\nrepresentations (INRs). INRs model time series as continuous functions and\nnaturally handle various missing data scenarios and sampling rates. While they\nhave shown strong performance within specific distributions, they struggle\nunder distribution shifts. To address this, we introduce MoTM (Mixture of\nTimeflow Models), a step toward a foundation model for time series imputation.\nBuilding on the idea that a new time series is a mixture of previously seen\npatterns, MoTM combines a basis of INRs, each trained independently on a\ndistinct family of time series, with a ridge regressor that adapts to the\nobserved context at inference. We demonstrate robust in-domain and\nout-of-domain generalization across diverse imputation scenarios (e.g., block\nand pointwise missingness, variable sampling rates), paving the way for\nadaptable foundation imputation models.","authors":["Etienne Le Naour","Tahar Nabil","Ghislain Agoua"],"url":"http://arxiv.org/pdf/2507.13207v1","published":"2025-07-17"}
{"title":"ChronoSelect: Robust Learning with Noisy Labels via Dynamics Temporal Memory","abstract":"Training deep neural networks on real-world datasets is often hampered by the\npresence of noisy labels, which can be memorized by over-parameterized models,\nleading to significant degradation in generalization performance. While\nexisting methods for learning with noisy labels (LNL) have made considerable\nprogress, they fundamentally suffer from static snapshot evaluations and fail\nto leverage the rich temporal dynamics of learning evolution. In this paper, we\npropose ChronoSelect (chrono denoting its temporal nature), a novel framework\nfeaturing an innovative four-stage memory architecture that compresses\nprediction history into compact temporal distributions. Our unique sliding\nupdate mechanism with controlled decay maintains only four dynamic memory units\nper sample, progressively emphasizing recent patterns while retaining essential\nhistorical knowledge. This enables precise three-way sample partitioning into\nclean, boundary, and noisy subsets through temporal trajectory analysis and\ndual-branch consistency. Theoretical guarantees prove the mechanism's\nconvergence and stability under noisy conditions. Extensive experiments\ndemonstrate ChronoSelect's state-of-the-art performance across synthetic and\nreal-world benchmarks.","authors":["Jianchao Wang","Qingfeng Li","Pengcheng Zheng","Xiaorong Pu","Yazhou Ren"],"url":"http://arxiv.org/pdf/2507.18183v1","published":"2025-07-24"}
{"title":"Observational Multiplicity","abstract":"Many prediction tasks can admit multiple models that can perform almost\nequally well. This phenomenon can can undermine interpretability and safety\nwhen competing models assign conflicting predictions to individuals. In this\nwork, we study how arbitrariness can arise in probabilistic classification\ntasks as a result of an effect that we call \\emph{observational multiplicity}.\nWe discuss how this effect arises in a broad class of practical applications\nwhere we learn a classifier to predict probabilities $p_i \\in [0,1]$ but are\ngiven a dataset of observations $y_i \\in \\{0,1\\}$. We propose to evaluate the\narbitrariness of individual probability predictions through the lens of\n\\emph{regret}. We introduce a measure of regret for probabilistic\nclassification tasks, which measures how the predictions of a model could\nchange as a result of different training labels change. We present a\ngeneral-purpose method to estimate the regret in a probabilistic classification\ntask. We use our measure to show that regret is higher for certain groups in\nthe dataset and discuss potential applications of regret. We demonstrate how\nestimating regret promote safety in real-world applications by abstention and\ndata collection.","authors":["Erin George","Deanna Needell","Berk Ustun"],"url":"http://arxiv.org/pdf/2507.23136v1","published":"2025-07-30"}
{"title":"Q-DPTS: Quantum Differentially Private Time Series Forecasting via Variational Quantum Circuits","abstract":"Time series forecasting is vital in domains where data sensitivity is\nparamount, such as finance and energy systems. While Differential Privacy (DP)\nprovides theoretical guarantees to protect individual data contributions, its\nintegration especially via DP-SGD often impairs model performance due to\ninjected noise. In this paper, we propose Q-DPTS, a hybrid quantum-classical\nframework for Quantum Differentially Private Time Series Forecasting. Q-DPTS\ncombines Variational Quantum Circuits (VQCs) with per-sample gradient clipping\nand Gaussian noise injection, ensuring rigorous $(\\epsilon,\n\\delta)$-differential privacy. The expressiveness of quantum models enables\nimproved robustness against the utility loss induced by DP mechanisms. We\nevaluate Q-DPTS on the ETT (Electricity Transformer Temperature) dataset, a\nstandard benchmark for long-term time series forecasting. Our approach is\ncompared against both classical and quantum baselines, including LSTM, QASA,\nQRWKV, and QLSTM. Results demonstrate that Q-DPTS consistently achieves lower\nprediction error under the same privacy budget, indicating a favorable\nprivacy-utility trade-off. This work presents one of the first explorations\ninto quantum-enhanced differentially private forecasting, offering promising\ndirections for secure and accurate time series modeling in privacy-critical\nscenarios.","authors":["Chi-Sheng Chen","Samuel Yen-Chi Chen"],"url":"http://arxiv.org/pdf/2508.05036v1","published":"2025-08-07"}
{"title":"Self-Supervised Temporal Super-Resolution of Energy Data using Generative Adversarial Transformer","abstract":"To bridge the temporal granularity gap in energy network design and operation\nbased on Energy System Models, resampling of time series is required. While\nconventional upsampling methods are computationally efficient, they often\nresult in significant information loss or increased noise. Advanced models such\nas time series generation models, Super-Resolution models and imputation models\nshow potential, but also face fundamental challenges. The goal of time series\ngenerative models is to learn the distribution of the original data to generate\nhigh-resolution series with similar statistical characteristics. This is not\nentirely consistent with the definition of upsampling. Time series\nSuper-Resolution models or imputation models can degrade the accuracy of\nupsampling because the input low-resolution time series are sparse and may have\ninsufficient context. Moreover, such models usually rely on supervised learning\nparadigms. This presents a fundamental application paradox: their training\nrequires the high-resolution time series that is intrinsically absent in\nupsampling application scenarios. To address the mentioned upsampling issue,\nthis paper introduces a new method utilizing Generative Adversarial\nTransformers (GATs), which can be trained without access to any ground-truth\nhigh-resolution data. Compared with conventional interpolation methods, the\nintroduced method can reduce the root mean square error (RMSE) of upsampling\ntasks by 9%, and the accuracy of a model predictive control (MPC) application\nscenario is improved by 13%.","authors":["Xuanhao Mu","G\u00f6khan Demirel","Yuzhe Zhang","Jianlei Liu","Thorsten Schlachter","Veit Hagenmeyer"],"url":"http://arxiv.org/pdf/2508.10587v1","published":"2025-08-14"}
{"title":"Enhancing Forecasting with a 2D Time Series Approach for Cohort-Based Data","abstract":"This paper introduces a novel two-dimensional (2D) time series forecasting\nmodel that integrates cohort behavior over time, addressing challenges in small\ndata environments. We demonstrate its efficacy using multiple real-world\ndatasets, showcasing superior performance in accuracy and adaptability compared\nto reference models. The approach offers valuable insights for strategic\ndecision-making across industries facing financial and marketing forecasting\nchallenges.","authors":["Yonathan Guttel","Orit Moradov","Nachi Lieder","Asnat Greenstein-Messica"],"url":"http://arxiv.org/pdf/2508.15369v1","published":"2025-08-21"}
{"title":"Online time series prediction using feature adjustment","abstract":"Time series forecasting is of significant importance across various domains.\nHowever, it faces significant challenges due to distribution shift. This issue\nbecomes particularly pronounced in online deployment scenarios where data\narrives sequentially, requiring models to adapt continually to evolving\npatterns. Current time series online learning methods focus on two main\naspects: selecting suitable parameters to update (e.g., final layer weights or\nadapter modules) and devising suitable update strategies (e.g., using recent\nbatches, replay buffers, or averaged gradients). We challenge the conventional\nparameter selection approach, proposing that distribution shifts stem from\nchanges in underlying latent factors influencing the data. Consequently,\nupdating the feature representations of these latent factors may be more\neffective. To address the critical problem of delayed feedback in multi-step\nforecasting (where true values arrive much later than predictions), we\nintroduce ADAPT-Z (Automatic Delta Adjustment via Persistent Tracking in\nZ-space). ADAPT-Z utilizes an adapter module that leverages current feature\nrepresentations combined with historical gradient information to enable robust\nparameter updates despite the delay. Extensive experiments demonstrate that our\nmethod consistently outperforms standard base models without adaptation and\nsurpasses state-of-the-art online learning approaches across multiple datasets.\nThe code is available at https://github.com/xiannanhuang/ADAPT-Z.","authors":["Xiannan Huang","Shuhan Qiu","Jiayuan Du","Chao Yang"],"url":"http://arxiv.org/pdf/2509.03810v1","published":"2025-09-04"}
{"title":"CountTRuCoLa: Rule Confidence Learning for Temporal Knowledge Graph Forecasting","abstract":"We address the task of temporal knowledge graph (TKG) forecasting by\nintroducing a fully explainable method based on temporal rules. Motivated by\nrecent work proposing a strong baseline using recurrent facts, our approach\nlearns four simple types of rules with a confidence function that considers\nboth recency and frequency. Evaluated on nine datasets, our method matches or\nsurpasses the performance of eight state-of-the-art models and two baselines,\nwhile providing fully interpretable predictions.","authors":["Julia Gastinger","Christian Meilicke","Heiner Stuckenschmidt"],"url":"http://arxiv.org/pdf/2509.09474v1","published":"2025-09-11"}
